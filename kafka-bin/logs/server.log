[2019-06-06 10:05:50,808] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:05:50,902] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:05:50,902] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:05:50,902] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:05:50,902] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-06-06 10:05:50,991] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:05:50,992] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-06-06 10:05:51,076] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,077] INFO Server environment:host.name=xqy-HP (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,078] INFO Server environment:java.version=1.8.0_111 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,079] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,080] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,081] INFO Server environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,084] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,085] INFO Server environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,085] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,086] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,087] INFO Server environment:os.arch=x86 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,087] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,090] INFO Server environment:user.name=xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,090] INFO Server environment:user.home=C:\Users\xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,091] INFO Server environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,397] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,397] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,398] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:05:51,665] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-06-06 10:05:51,679] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:07:18,257] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 10:07:20,821] INFO starting (kafka.server.KafkaServer)
[2019-06-06 10:07:20,822] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 10:07:20,955] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:07:20,977] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,977] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,978] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,978] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,978] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,979] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,981] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,982] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,982] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,983] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,983] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,984] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,984] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,985] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,986] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:20,990] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@130745 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:07:21,078] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:07:21,079] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:07:21,081] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:07:21,081] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:64331 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:07:21,110] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:64331 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:07:21,112] INFO Creating new log file: log.199 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-06-06 10:07:21,179] INFO Established session 0x1000c6f0c3e0000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:64331 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:07:21,182] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000c6f0c3e0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:07:21,186] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:07:21,593] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x1 zxid:0x19a txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,727] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x2 zxid:0x19b txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,821] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x3 zxid:0x19c txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,856] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x4 zxid:0x19d txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,888] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x5 zxid:0x19e txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,930] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x6 zxid:0x19f txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:21,977] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x7 zxid:0x1a0 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,019] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x8 zxid:0x1a1 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,066] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0x9 zxid:0x1a2 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,141] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0xa zxid:0x1a3 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,188] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0xb zxid:0x1a4 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,230] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0xc zxid:0x1a5 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,333] INFO Got user-level KeeperException when processing sessionid:0x1000c6f0c3e0000 type:create cxid:0xd zxid:0x1a6 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:07:22,920] INFO Cluster ID = nZyUWUdKQ3yvmijW4epzyw (kafka.server.KafkaServer)
[2019-06-06 10:07:23,192] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:07:23,205] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:07:23,781] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:07:23,781] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:07:23,822] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:07:24,161] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 10:07:24,779] WARN [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559717211923}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:24,781] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:24,878] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 5274 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:26,728] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:26,731] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 5274 (kafka.log.Log)
[2019-06-06 10:07:26,733] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:26,738] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:27,028] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:27,268] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 6044 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,272] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000006044.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:27,275] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 5274 and log end offset 6044 in 2775 ms (kafka.log.Log)
[2019-06-06 10:07:27,515] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 76 (kafka.log.Log)
[2019-06-06 10:07:27,516] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,519] INFO [ProducerStateManager partition=javainuse-topic-0] Writing producer snapshot at offset 76 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:27,590] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,593] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 76 and log end offset 76 in 305 ms (kafka.log.Log)
[2019-06-06 10:07:27,648] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:27,648] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,661] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,663] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 68 ms (kafka.log.Log)
[2019-06-06 10:07:27,705] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:27,706] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,712] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,715] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-06-06 10:07:27,789] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:27,789] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,823] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,826] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 106 ms (kafka.log.Log)
[2019-06-06 10:07:27,840] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:27,841] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,862] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,865] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[2019-06-06 10:07:27,914] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:27,915] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,945] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:27,948] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-06-06 10:07:28,001] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,002] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,065] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,068] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 117 ms (kafka.log.Log)
[2019-06-06 10:07:28,201] WARN [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553753082425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:28,201] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,234] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:28,236] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,236] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,243] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:28,369] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,372] INFO [ProducerStateManager partition=__consumer_offsets-11] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:28,372] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 272 ms (kafka.log.Log)
[2019-06-06 10:07:28,441] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,441] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,447] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,449] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 74 ms (kafka.log.Log)
[2019-06-06 10:07:28,518] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,519] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,570] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,573] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 121 ms (kafka.log.Log)
[2019-06-06 10:07:28,635] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,636] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,676] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,678] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 103 ms (kafka.log.Log)
[2019-06-06 10:07:28,725] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,725] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,761] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,763] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[2019-06-06 10:07:28,854] WARN [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:28,854] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,860] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:28,863] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:28,863] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:28,872] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,028] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,030] INFO [ProducerStateManager partition=__consumer_offsets-16] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,031] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 266 ms (kafka.log.Log)
[2019-06-06 10:07:29,039] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,039] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,059] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,061] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[2019-06-06 10:07:29,136] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,137] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,143] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,146] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[2019-06-06 10:07:29,184] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,185] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,197] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,199] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[2019-06-06 10:07:29,251] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,251] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,257] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,260] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-06-06 10:07:29,325] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,325] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,363] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,366] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 104 ms (kafka.log.Log)
[2019-06-06 10:07:29,374] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,375] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,380] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,383] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-06-06 10:07:29,399] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,399] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,406] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,409] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-06-06 10:07:29,484] WARN [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082436}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:29,485] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,513] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,514] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:29,514] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,520] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,686] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,688] INFO [ProducerStateManager partition=__consumer_offsets-23] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,689] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 278 ms (kafka.log.Log)
[2019-06-06 10:07:29,749] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.index.swap from interrupted swap operation. (kafka.log.Log)
[2019-06-06 10:07:29,749] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix  for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.index (kafka.log.Log)
[2019-06-06 10:07:29,770] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log.swap from interrupted swap operation. (kafka.log.Log)
[2019-06-06 10:07:29,770] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix  for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log (kafka.log.Log)
[2019-06-06 10:07:29,785] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix .swap for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log (kafka.log.Log)
[2019-06-06 10:07:29,811] WARN [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559720384087}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:29,812] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,815] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,852] INFO [ProducerStateManager partition=__consumer_offsets-24] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,854] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Recovering unflushed segment 3 (kafka.log.Log)
[2019-06-06 10:07:29,855] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:29,857] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,864] INFO [ProducerStateManager partition=__consumer_offsets-24] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:29,997] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,019] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000004.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:30,019] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Completed load of log with 2 segments, log start offset 0 and log end offset 4 in 329 ms (kafka.log.Log)
[2019-06-06 10:07:30,093] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,093] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,139] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,142] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 120 ms (kafka.log.Log)
[2019-06-06 10:07:30,230] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,230] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,237] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,240] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 96 ms (kafka.log.Log)
[2019-06-06 10:07:30,271] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,271] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,298] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,301] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-06-06 10:07:30,340] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,340] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,346] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,348] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-06-06 10:07:30,390] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,390] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,429] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,432] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 82 ms (kafka.log.Log)
[2019-06-06 10:07:30,513] WARN [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082427}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:30,513] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,519] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:30,521] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,521] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,528] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:30,730] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,733] INFO [ProducerStateManager partition=__consumer_offsets-3] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:30,734] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 300 ms (kafka.log.Log)
[2019-06-06 10:07:30,833] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,834] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,863] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,866] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 131 ms (kafka.log.Log)
[2019-06-06 10:07:30,883] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:30,884] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,897] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:30,900] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[2019-06-06 10:07:31,001] WARN [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682424}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:31,001] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,006] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,007] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,008] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,013] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,186] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,191] INFO [ProducerStateManager partition=__consumer_offsets-32] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,191] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 289 ms (kafka.log.Log)
[2019-06-06 10:07:31,259] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,260] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,305] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,307] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 114 ms (kafka.log.Log)
[2019-06-06 10:07:31,381] WARN [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553756682423}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:31,381] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,403] INFO [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,404] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,405] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,409] INFO [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,523] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,526] INFO [ProducerStateManager partition=__consumer_offsets-34] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000006.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,527] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 6 in 218 ms (kafka.log.Log)
[2019-06-06 10:07:31,590] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,593] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,637] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,641] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 113 ms (kafka.log.Log)
[2019-06-06 10:07:31,724] WARN [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082439}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:31,724] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,729] INFO [ProducerStateManager partition=__consumer_offsets-36] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,730] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,730] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,736] INFO [ProducerStateManager partition=__consumer_offsets-36] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,909] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:31,912] INFO [ProducerStateManager partition=__consumer_offsets-36] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:31,914] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 272 ms (kafka.log.Log)
[2019-06-06 10:07:31,984] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:31,986] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,016] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,018] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 103 ms (kafka.log.Log)
[2019-06-06 10:07:32,062] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,062] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,089] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,093] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 72 ms (kafka.log.Log)
[2019-06-06 10:07:32,130] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,130] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,136] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,138] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-06-06 10:07:32,196] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,197] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,204] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,207] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 68 ms (kafka.log.Log)
[2019-06-06 10:07:32,270] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,270] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,275] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,277] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 69 ms (kafka.log.Log)
[2019-06-06 10:07:32,341] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,341] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,346] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,349] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 71 ms (kafka.log.Log)
[2019-06-06 10:07:32,371] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,371] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,377] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,380] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 30 ms (kafka.log.Log)
[2019-06-06 10:07:32,389] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,389] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,414] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,417] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-06-06 10:07:32,483] WARN [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553818654937}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:32,484] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,508] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 15 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:32,509] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,509] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,516] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 15 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:32,653] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 15 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,655] INFO [ProducerStateManager partition=__consumer_offsets-44] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000015.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:32,655] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 15 in 237 ms (kafka.log.Log)
[2019-06-06 10:07:32,699] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,701] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,710] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,713] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-06-06 10:07:32,750] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,750] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,755] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,757] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 41 ms (kafka.log.Log)
[2019-06-06 10:07:32,818] WARN [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553751882423}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:32,818] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,824] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:32,826] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:32,826] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:32,855] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,009] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,012] INFO [ProducerStateManager partition=__consumer_offsets-47] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,012] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 253 ms (kafka.log.Log)
[2019-06-06 10:07:33,085] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,143] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,149] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,151] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 138 ms (kafka.log.Log)
[2019-06-06 10:07:33,253] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,254] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,260] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,262] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 110 ms (kafka.log.Log)
[2019-06-06 10:07:33,353] WARN [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682426}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:33,354] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,359] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,360] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,361] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,384] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,628] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,631] INFO [ProducerStateManager partition=__consumer_offsets-5] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,631] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 368 ms (kafka.log.Log)
[2019-06-06 10:07:33,731] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,731] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,767] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,769] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 137 ms (kafka.log.Log)
[2019-06-06 10:07:33,820] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,820] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,826] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,838] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 67 ms (kafka.log.Log)
[2019-06-06 10:07:33,928] WARN [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559720196754}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:07:33,928] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,941] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:33,943] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:33,943] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:33,950] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-06-06 10:07:34,144] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:34,147] INFO [ProducerStateManager partition=__consumer_offsets-8] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:07:34,147] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 304 ms (kafka.log.Log)
[2019-06-06 10:07:34,237] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:07:34,237] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:34,285] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:07:34,293] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 145 ms (kafka.log.Log)
[2019-06-06 10:07:34,297] INFO Logs loading complete in 10136 ms. (kafka.log.LogManager)
[2019-06-06 10:07:34,413] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-06-06 10:07:34,414] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-06-06 10:07:36,789] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned -> C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.swap: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:387)
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
	at java.nio.file.Files.move(Files.java:1395)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:805)
	at kafka.log.AbstractIndex.renameTo(AbstractIndex.scala:205)
	at kafka.log.LogSegment.changeFileSuffixes(LogSegment.scala:490)
	at kafka.log.Log.$anonfun$replaceSegments$4(Log.scala:1978)
	at kafka.log.Log.$anonfun$replaceSegments$4$adapted(Log.scala:1978)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Log.replaceSegments(Log.scala:1978)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:580)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
	Suppressed: java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned -> C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.swap: 另一个程序正在使用此文件，进程无法访问。

		at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
		at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
		at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
		at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
		at java.nio.file.Files.move(Files.java:1395)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:802)
		... 16 more
[2019-06-06 10:07:37,461] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:37,887] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[2019-06-06 10:07:37,931] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:38,555] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:39,283] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:39,440] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:39,536] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-06-06 10:07:39,538] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-06-06 10:07:39,631] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:39,784] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:39,911] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:40,166] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:07:40,167] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:07:40,166] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:07:40,167] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:07:40,169] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:40,390] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:40,463] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 10:07:40,465] INFO [ReplicaManager broker=0] Stopping serving replicas in dir C:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-06-06 10:07:40,552] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-06-06 10:07:40,567] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 10:07:40,568] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:40,573] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory C:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-06-06 10:07:40,574] INFO Stopping serving logs in dir C:\tmp\kafka-logs (kafka.log.LogManager)
[2019-06-06 10:07:40,750] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:40,897] ERROR Shutdown broker because all log dirs in C:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-06-06 10:07:40,911] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:07:41,428] WARN Exception causing close of session 0x1000c6f0c3e0000: 远程主机强迫关闭了一个现有的连接。 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:07:41,429] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:64331 which had sessionid 0x1000c6f0c3e0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:07:46,950] INFO Expiring session 0x1000c6f0c3e0000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:07:46,950] INFO Processed session termination for sessionid: 0x1000c6f0c3e0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:15:58,535] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:15:58,618] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:15:58,618] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:15:58,619] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:15:58,619] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-06-06 10:15:58,683] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:15:58,687] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-06-06 10:15:58,770] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,771] INFO Server environment:host.name=xqy-HP (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,771] INFO Server environment:java.version=1.8.0_111 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,772] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,773] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,774] INFO Server environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,777] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,777] INFO Server environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,778] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,778] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,779] INFO Server environment:os.arch=x86 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,780] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,782] INFO Server environment:user.name=xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,783] INFO Server environment:user.home=C:\Users\xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,783] INFO Server environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,824] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,824] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:58,826] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:15:59,538] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-06-06 10:15:59,598] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:16:15,791] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 10:16:19,698] INFO starting (kafka.server.KafkaServer)
[2019-06-06 10:16:19,699] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 10:16:19,964] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:16:19,992] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,993] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,993] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,993] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,993] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,994] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,996] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,996] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,997] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,998] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,998] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,999] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:19,999] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:20,000] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:20,001] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:20,006] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@130745 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:16:20,145] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:16:20,181] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:16:20,184] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:16:20,183] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:64785 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:16:20,213] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:64785 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:16:20,249] INFO Creating new log file: log.1a8 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-06-06 10:16:20,504] INFO Established session 0x1000c78622c0000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:64785 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:16:20,507] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000c78622c0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:16:20,512] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:16:20,817] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x1 zxid:0x1a9 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:20,952] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x2 zxid:0x1aa txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:20,997] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x3 zxid:0x1ab txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,053] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x4 zxid:0x1ac txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,122] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x5 zxid:0x1ad txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,197] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x6 zxid:0x1ae txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,398] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x7 zxid:0x1af txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,687] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x8 zxid:0x1b0 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,853] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0x9 zxid:0x1b1 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:21,954] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0xa zxid:0x1b2 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:22,009] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0xb zxid:0x1b3 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:22,075] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0xc zxid:0x1b4 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:22,110] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0000 type:create cxid:0xd zxid:0x1b5 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:16:22,914] INFO Cluster ID = nZyUWUdKQ3yvmijW4epzyw (kafka.server.KafkaServer)
[2019-06-06 10:16:23,288] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:16:23,305] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:16:23,449] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:16:23,449] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:16:23,452] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:16:23,762] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 10:16:24,158] WARN [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559717211923}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:24,160] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:24,213] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 5274 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:36,463] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:36,474] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 5274 (kafka.log.Log)
[2019-06-06 10:16:36,477] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:36,480] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:37,185] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:38,311] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 6044 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:38,315] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000006044.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:38,318] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 5274 and log end offset 6044 in 14331 ms (kafka.log.Log)
[2019-06-06 10:16:38,927] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 76 (kafka.log.Log)
[2019-06-06 10:16:38,991] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:38,993] INFO [ProducerStateManager partition=javainuse-topic-0] Writing producer snapshot at offset 76 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:38,996] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:38,998] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 76 and log end offset 76 in 659 ms (kafka.log.Log)
[2019-06-06 10:16:39,584] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:39,584] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:39,595] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:39,597] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 595 ms (kafka.log.Log)
[2019-06-06 10:16:41,038] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:41,038] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:41,044] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:41,047] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1446 ms (kafka.log.Log)
[2019-06-06 10:16:41,803] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:41,803] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:41,995] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,001] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 949 ms (kafka.log.Log)
[2019-06-06 10:16:42,571] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:42,572] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,576] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,579] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 573 ms (kafka.log.Log)
[2019-06-06 10:16:42,742] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:42,743] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,750] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,753] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 171 ms (kafka.log.Log)
[2019-06-06 10:16:42,939] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:42,939] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,944] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:42,947] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 192 ms (kafka.log.Log)
[2019-06-06 10:16:43,281] WARN [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553753082425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:43,281] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,287] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:43,288] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,289] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,294] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:43,511] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,514] INFO [ProducerStateManager partition=__consumer_offsets-11] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:43,515] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 512 ms (kafka.log.Log)
[2019-06-06 10:16:43,610] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,610] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,618] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,621] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 102 ms (kafka.log.Log)
[2019-06-06 10:16:43,720] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,720] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,727] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,731] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 107 ms (kafka.log.Log)
[2019-06-06 10:16:43,784] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,784] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,788] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,790] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-06-06 10:16:43,808] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,808] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,814] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,816] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-06-06 10:16:43,935] WARN [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:43,935] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,940] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:43,941] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:43,941] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:43,948] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,088] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,091] INFO [ProducerStateManager partition=__consumer_offsets-16] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,092] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 274 ms (kafka.log.Log)
[2019-06-06 10:16:44,139] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,139] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,144] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,147] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-06-06 10:16:44,176] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,177] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,182] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,185] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[2019-06-06 10:16:44,260] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,261] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,267] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,270] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[2019-06-06 10:16:44,310] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,310] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,315] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,318] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-06-06 10:16:44,353] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,357] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,363] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,366] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-06-06 10:16:44,374] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,374] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,381] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,384] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2019-06-06 10:16:44,477] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,477] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,483] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,486] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 99 ms (kafka.log.Log)
[2019-06-06 10:16:44,520] WARN [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082436}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:44,520] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,525] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,525] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:44,526] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,531] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,636] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,638] INFO [ProducerStateManager partition=__consumer_offsets-23] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-23\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,639] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 151 ms (kafka.log.Log)
[2019-06-06 10:16:44,767] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.index.swap from interrupted swap operation. (kafka.log.Log)
[2019-06-06 10:16:44,790] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix  for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.index (kafka.log.Log)
[2019-06-06 10:16:44,830] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log.swap from interrupted swap operation. (kafka.log.Log)
[2019-06-06 10:16:44,830] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix  for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log (kafka.log.Log)
[2019-06-06 10:16:44,839] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Deleting index files with suffix .swap for baseFile C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.log (kafka.log.Log)
[2019-06-06 10:16:44,869] WARN [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559720384087}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:44,869] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,873] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,912] INFO [ProducerStateManager partition=__consumer_offsets-24] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,914] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Recovering unflushed segment 3 (kafka.log.Log)
[2019-06-06 10:16:44,914] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:44,917] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:44,919] INFO [ProducerStateManager partition=__consumer_offsets-24] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,021] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 4 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,025] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000004.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,025] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Completed load of log with 2 segments, log start offset 0 and log end offset 4 in 384 ms (kafka.log.Log)
[2019-06-06 10:16:45,059] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,059] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,065] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,067] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-06-06 10:16:45,142] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,145] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,151] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,156] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 87 ms (kafka.log.Log)
[2019-06-06 10:16:45,206] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,207] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,214] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,218] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 58 ms (kafka.log.Log)
[2019-06-06 10:16:45,282] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,283] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,288] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,290] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 63 ms (kafka.log.Log)
[2019-06-06 10:16:45,341] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,342] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,349] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,353] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 61 ms (kafka.log.Log)
[2019-06-06 10:16:45,470] WARN [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082427}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:45,470] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,477] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,481] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,482] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,550] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,673] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,675] INFO [ProducerStateManager partition=__consumer_offsets-3] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-3\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,675] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 320 ms (kafka.log.Log)
[2019-06-06 10:16:45,735] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,735] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,740] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,743] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 10:16:45,772] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,772] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,778] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,781] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[2019-06-06 10:16:45,857] WARN [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682424}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:45,858] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,863] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,864] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:45,865] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:45,869] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:45,999] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,001] INFO [ProducerStateManager partition=__consumer_offsets-32] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-32\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,002] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 219 ms (kafka.log.Log)
[2019-06-06 10:16:46,073] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,073] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,080] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,083] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 79 ms (kafka.log.Log)
[2019-06-06 10:16:46,139] WARN [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553756682423}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:46,139] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,161] INFO [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,162] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,162] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,167] INFO [ProducerStateManager partition=__consumer_offsets-34] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,234] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 6 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,237] INFO [ProducerStateManager partition=__consumer_offsets-34] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-34\00000000000000000006.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,237] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 6 in 153 ms (kafka.log.Log)
[2019-06-06 10:16:46,263] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,264] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,270] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,273] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-06-06 10:16:46,359] WARN [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750082439}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:46,359] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,365] INFO [ProducerStateManager partition=__consumer_offsets-36] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,367] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,367] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,373] INFO [ProducerStateManager partition=__consumer_offsets-36] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,504] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,518] INFO [ProducerStateManager partition=__consumer_offsets-36] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-36\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:46,519] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 244 ms (kafka.log.Log)
[2019-06-06 10:16:46,639] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,640] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,647] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,650] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 129 ms (kafka.log.Log)
[2019-06-06 10:16:46,721] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,721] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,730] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,737] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 86 ms (kafka.log.Log)
[2019-06-06 10:16:46,801] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,804] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,814] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,820] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-06-06 10:16:46,878] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,879] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,886] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,893] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 71 ms (kafka.log.Log)
[2019-06-06 10:16:46,933] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:46,940] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,965] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:46,968] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 70 ms (kafka.log.Log)
[2019-06-06 10:16:47,212] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,214] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,373] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,376] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 406 ms (kafka.log.Log)
[2019-06-06 10:16:47,404] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,404] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,411] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,414] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-06-06 10:16:47,466] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,466] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,471] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,473] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 57 ms (kafka.log.Log)
[2019-06-06 10:16:47,707] WARN [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553818654937}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:47,709] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,754] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 15 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:47,755] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,756] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,763] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 15 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:47,926] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 15 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,930] INFO [ProducerStateManager partition=__consumer_offsets-44] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-44\00000000000000000015.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:47,930] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 15 in 456 ms (kafka.log.Log)
[2019-06-06 10:16:47,966] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,966] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,971] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:47,974] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[2019-06-06 10:16:47,998] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:47,998] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,003] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,005] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 30 ms (kafka.log.Log)
[2019-06-06 10:16:48,210] WARN [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553751882423}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:48,212] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,218] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,220] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:48,221] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,232] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,378] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,381] INFO [ProducerStateManager partition=__consumer_offsets-47] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-47\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,382] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 376 ms (kafka.log.Log)
[2019-06-06 10:16:48,564] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:48,565] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,569] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,571] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 188 ms (kafka.log.Log)
[2019-06-06 10:16:48,641] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:48,643] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,650] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,652] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 80 ms (kafka.log.Log)
[2019-06-06 10:16:48,734] WARN [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682426}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:48,734] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,738] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,740] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:48,740] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,749] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,904] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:48,907] INFO [ProducerStateManager partition=__consumer_offsets-5] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-5\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:48,908] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 255 ms (kafka.log.Log)
[2019-06-06 10:16:49,065] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:49,066] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,070] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,073] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 163 ms (kafka.log.Log)
[2019-06-06 10:16:49,169] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:49,170] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,178] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,182] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 107 ms (kafka.log.Log)
[2019-06-06 10:16:49,430] WARN [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559720196754}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:16:49,430] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,499] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:49,500] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:49,501] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,505] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-06-06 10:16:49,632] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,635] INFO [ProducerStateManager partition=__consumer_offsets-8] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:16:49,635] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 452 ms (kafka.log.Log)
[2019-06-06 10:16:49,731] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:16:49,731] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,735] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:16:49,737] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 101 ms (kafka.log.Log)
[2019-06-06 10:16:49,742] INFO Logs loading complete in 25980 ms. (kafka.log.LogManager)
[2019-06-06 10:16:49,837] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-06-06 10:16:49,838] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-06-06 10:16:54,283] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned -> C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.swap: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:387)
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
	at java.nio.file.Files.move(Files.java:1395)
	at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:805)
	at kafka.log.AbstractIndex.renameTo(AbstractIndex.scala:205)
	at kafka.log.LogSegment.changeFileSuffixes(LogSegment.scala:490)
	at kafka.log.Log.$anonfun$replaceSegments$4(Log.scala:1978)
	at kafka.log.Log.$anonfun$replaceSegments$4$adapted(Log.scala:1978)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Log.replaceSegments(Log.scala:1978)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:580)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
	Suppressed: java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned -> C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.swap: 另一个程序正在使用此文件，进程无法访问。

		at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
		at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
		at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
		at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
		at java.nio.file.Files.move(Files.java:1395)
		at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:802)
		... 16 more
[2019-06-06 10:16:54,880] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:55,190] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:55,489] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:55,869] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:56,224] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:56,772] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:57,502] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:58,456] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:58,643] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[2019-06-06 10:16:59,017] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:59,351] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:59,644] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:16:59,671] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-06-06 10:16:59,673] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-06-06 10:16:59,985] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:17:00,234] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:17:00,590] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:17:00,672] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:17:00,673] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:17:00,674] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:17:00,674] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:17:00,757] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 10:17:00,763] INFO [ReplicaManager broker=0] Stopping serving replicas in dir C:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-06-06 10:17:00,931] ERROR Failed to clean up log for __consumer_offsets-24 in dir C:\tmp\kafka-logs due to IOException (kafka.server.LogDirFailureChannel)
java.nio.file.FileSystemException: C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000000.timeindex.cleaned: 另一个程序正在使用此文件，进程无法访问。

	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at kafka.log.Log$.deleteFileIfExists(Log.scala:2236)
	at kafka.log.LogSegment$.deleteIfExists(LogSegment.scala:643)
	at kafka.log.LogCleaner$.createNewCleanedSegment(LogCleaner.scala:421)
	at kafka.log.Cleaner.cleanSegments(LogCleaner.scala:537)
	at kafka.log.Cleaner.$anonfun$doClean$6(LogCleaner.scala:512)
	at kafka.log.Cleaner.$anonfun$doClean$6$adapted(LogCleaner.scala:511)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at kafka.log.Cleaner.doClean(LogCleaner.scala:511)
	at kafka.log.Cleaner.clean(LogCleaner.scala:489)
	at kafka.log.LogCleaner$CleanerThread.cleanLog(LogCleaner.scala:350)
	at kafka.log.LogCleaner$CleanerThread.cleanFilthiestLog(LogCleaner.scala:319)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:300)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-06-06 10:17:00,937] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-06-06 10:17:00,949] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 10:17:00,957] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions  and stopped moving logs for partitions  because they are in the failed log directory C:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-06-06 10:17:00,958] INFO Stopping serving logs in dir C:\tmp\kafka-logs (kafka.log.LogManager)
[2019-06-06 10:17:01,026] ERROR Shutdown broker because all log dirs in C:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-06-06 10:17:01,897] WARN Exception causing close of session 0x1000c78622c0000: 远程主机强迫关闭了一个现有的连接。 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:17:01,898] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:64785 which had sessionid 0x1000c78622c0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:17:07,587] INFO Expiring session 0x1000c78622c0000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:17:07,589] INFO Processed session termination for sessionid: 0x1000c78622c0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:02,655] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 10:23:04,644] INFO starting (kafka.server.KafkaServer)
[2019-06-06 10:23:04,645] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 10:23:04,822] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:23:04,856] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,856] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,857] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,857] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,857] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,857] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,858] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,859] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,860] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,860] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:04,861] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@130745 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:23:05,020] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:23:05,020] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:23:05,022] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:65463 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:23:05,023] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:23:05,034] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:65463 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:23:05,083] INFO Established session 0x1000c78622c0001 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:65463 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:23:05,085] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000c78622c0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:23:05,090] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:23:06,380] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x1 zxid:0x1b8 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,104] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x2 zxid:0x1b9 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,285] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x3 zxid:0x1ba txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,439] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x4 zxid:0x1bb txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,562] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x5 zxid:0x1bc txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,663] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x6 zxid:0x1bd txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,752] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x7 zxid:0x1be txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,873] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x8 zxid:0x1bf txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:07,951] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0x9 zxid:0x1c0 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:08,042] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0xa zxid:0x1c1 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:08,117] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0xb zxid:0x1c2 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:08,161] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0xc zxid:0x1c3 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:08,250] INFO Got user-level KeeperException when processing sessionid:0x1000c78622c0001 type:create cxid:0xd zxid:0x1c4 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:23:10,369] INFO Cluster ID = nZyUWUdKQ3yvmijW4epzyw (kafka.server.KafkaServer)
[2019-06-06 10:23:10,969] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:23:10,980] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:23:11,348] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:23:11,348] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:23:11,350] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:23:11,936] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 10:23:13,344] WARN [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1559717211923}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:23:13,347] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:13,433] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 5274 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:20,898] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:26,995] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 5274 (kafka.log.Log)
[2019-06-06 10:23:26,995] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 5274 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:26,998] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000005274.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:23:27,215] INFO [ProducerStateManager partition=distributed-video1-0] Writing producer snapshot at offset 6044 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:27,449] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 6044 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,451] INFO [ProducerStateManager partition=distributed-video1-0] Loading producer state from snapshot file 'C:\tmp\kafka-logs\distributed-video1-0\00000000000000006044.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:23:27,453] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 5274 and log end offset 6044 in 14668 ms (kafka.log.Log)
[2019-06-06 10:23:27,469] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 76 (kafka.log.Log)
[2019-06-06 10:23:27,469] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,472] INFO [ProducerStateManager partition=javainuse-topic-0] Writing producer snapshot at offset 76 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:27,522] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 76 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,524] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 76 and log end offset 76 in 62 ms (kafka.log.Log)
[2019-06-06 10:23:27,569] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,569] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,604] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,607] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-06-06 10:23:27,668] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,669] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,698] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,701] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 92 ms (kafka.log.Log)
[2019-06-06 10:23:27,755] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,755] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,785] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,788] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 84 ms (kafka.log.Log)
[2019-06-06 10:23:27,806] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,807] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,812] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,814] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-06-06 10:23:27,869] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,870] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,896] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,899] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 80 ms (kafka.log.Log)
[2019-06-06 10:23:27,936] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:27,936] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,941] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:27,944] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[2019-06-06 10:23:28,144] WARN [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553753082425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:23:28,144] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,149] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,150] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,151] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,173] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,265] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,267] INFO [ProducerStateManager partition=__consumer_offsets-11] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-11\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,268] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 290 ms (kafka.log.Log)
[2019-06-06 10:23:28,336] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,337] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,372] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,375] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 105 ms (kafka.log.Log)
[2019-06-06 10:23:28,427] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,428] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,462] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,465] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 88 ms (kafka.log.Log)
[2019-06-06 10:23:28,501] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,502] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,547] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,549] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[2019-06-06 10:23:28,625] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,625] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,654] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,657] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 106 ms (kafka.log.Log)
[2019-06-06 10:23:28,720] WARN [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Found a corrupted index file corresponding to log file C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.log due to Corrupt time index found, time index file (C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1553750682425}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-06-06 10:23:28,720] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,746] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,748] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:28,748] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,783] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,934] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:28,936] INFO [ProducerStateManager partition=__consumer_offsets-16] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-16\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 10:23:28,936] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 278 ms (kafka.log.Log)
[2019-06-06 10:23:29,090] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:29,090] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:29,134] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:29,137] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 199 ms (kafka.log.Log)
[2019-06-06 10:23:29,175] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-06-06 10:23:29,175] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:29,195] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-06-06 10:23:29,199] ERROR [KafkaServer id=0] Fatal error during KafkaServer shutdown. (kafka.server.KafkaServer)
java.lang.IllegalStateException: Kafka server is still starting up, cannot shut down!
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:579)
	at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:48)
	at kafka.Kafka$$anon$1.run(Kafka.scala:72)
[2019-06-06 10:23:29,200] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:23:29,352] ERROR Halting Kafka. (kafka.server.KafkaServerStartable)
[2019-06-06 10:23:29,791] WARN Exception causing close of session 0x1000c78622c0001: 远程主机强迫关闭了一个现有的连接。 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:23:29,792] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:65463 which had sessionid 0x1000c78622c0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 10:23:34,587] INFO Expiring session 0x1000c78622c0001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:23:34,587] INFO Processed session termination for sessionid: 0x1000c78622c0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:30,739] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:27:30,773] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:27:30,773] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:27:30,773] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 10:27:30,774] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-06-06 10:27:30,789] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 10:27:30,790] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-06-06 10:27:30,800] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,800] INFO Server environment:host.name=xqy-HP (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,800] INFO Server environment:java.version=1.8.0_111 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,801] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,801] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,801] INFO Server environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,803] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:os.arch=x86 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:user.name=xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:user.home=C:\Users\xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,804] INFO Server environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,854] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,855] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,855] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:30,941] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-06-06 10:27:30,944] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:27:41,806] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 10:27:42,470] INFO starting (kafka.server.KafkaServer)
[2019-06-06 10:27:42,471] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 10:27:42,494] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:27:42,500] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,500] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,500] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,501] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,501] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,501] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,502] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,502] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,502] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,502] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,503] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,504] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,504] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,504] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,504] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,505] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@130745 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 10:27:42,532] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:27:42,533] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:27:42,535] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:27:42,535] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:49531 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 10:27:42,566] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:49531 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:42,567] INFO Creating new log file: log.1c6 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-06-06 10:27:42,705] INFO Established session 0x1000c82ec300000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:49531 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 10:27:42,707] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000c82ec300000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 10:27:42,710] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 10:27:42,759] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x1 zxid:0x1c7 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,813] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x2 zxid:0x1c8 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,843] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x3 zxid:0x1c9 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,863] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x4 zxid:0x1ca txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,886] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x5 zxid:0x1cb txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,907] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x6 zxid:0x1cc txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,931] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x7 zxid:0x1cd txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,952] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x8 zxid:0x1ce txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,975] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0x9 zxid:0x1cf txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:42,997] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0xa zxid:0x1d0 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:43,020] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0xb zxid:0x1d1 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:43,041] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0xc zxid:0x1d2 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:43,063] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:create cxid:0xd zxid:0x1d3 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:43,243] INFO Cluster ID = nZyUWUdKQ3yvmijW4epzyw (kafka.server.KafkaServer)
[2019-06-06 10:27:43,247] WARN No meta.properties file under dir C:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-06-06 10:27:43,320] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:27:43,331] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 10:27:43,360] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:27:43,361] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:27:43,362] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 10:27:43,399] INFO Log directory C:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-06-06 10:27:43,420] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 10:27:43,430] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-06-06 10:27:43,502] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-06-06 10:27:43,507] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-06-06 10:27:44,484] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[2019-06-06 10:27:44,632] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-06-06 10:27:44,634] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-06-06 10:27:44,767] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:44,768] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:44,769] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:44,770] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:44,804] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 10:27:45,168] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-06-06 10:27:45,426] INFO Stat of the created znode at /brokers/ids/0 is: 468,468,1559788065375,1559788065375,1,0,0,72071350485778432,182,0,468
 (kafka.zk.KafkaZkClient)
[2019-06-06 10:27:45,428] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(xqy-HP,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 468 (kafka.zk.KafkaZkClient)
[2019-06-06 10:27:45,430] WARN No meta.properties file under dir C:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-06-06 10:27:45,601] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:45,639] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:45,639] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 10:27:45,814] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:27:45,816] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:27:45,852] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 36 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:45,998] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:6000,blockEndProducerId:6999) by writing to Zk with path version 7 (kafka.coordinator.transaction.ProducerIdManager)
[2019-06-06 10:27:46,226] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 10:27:46,227] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 10:27:46,227] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 10:27:46,592] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 10:27:46,723] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-06-06 10:27:46,811] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:multi cxid:0x77 zxid:0x1d7 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:27:46,974] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 10:27:46,978] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 10:27:46,982] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-06-06 10:27:47,070] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-plaintext-input-0, __consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, test-0, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, distributed-video1-0, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, javainuse-topic-0, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, streams-wordcount-output-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-06-06 10:27:47,261] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:47,320] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 216 ms (kafka.log.Log)
[2019-06-06 10:27:47,331] INFO Created log for partition __consumer_offsets-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:47,332] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-06-06 10:27:47,334] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:47,340] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:47,951] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:47,954] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 281 ms (kafka.log.Log)
[2019-06-06 10:27:47,955] INFO Created log for partition __consumer_offsets-29 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:47,955] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-06-06 10:27:47,956] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:47,956] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:48,296] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:48,299] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 165 ms (kafka.log.Log)
[2019-06-06 10:27:48,300] INFO Created log for partition __consumer_offsets-48 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:48,300] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-06-06 10:27:48,300] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:48,300] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:48,454] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:48,457] INFO [Log partition=javainuse-topic-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-06-06 10:27:48,457] INFO Created log for partition javainuse-topic-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:48,458] INFO [Partition javainuse-topic-0 broker=0] No checkpointed highwatermark is found for partition javainuse-topic-0 (kafka.cluster.Partition)
[2019-06-06 10:27:48,458] INFO Replica loaded for partition javainuse-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:48,458] INFO [Partition javainuse-topic-0 broker=0] javainuse-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:48,621] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:48,623] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-06-06 10:27:48,624] INFO Created log for partition __consumer_offsets-10 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:48,625] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-06-06 10:27:48,625] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:48,625] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:48,828] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:48,830] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 86 ms (kafka.log.Log)
[2019-06-06 10:27:48,832] INFO Created log for partition __consumer_offsets-45 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:48,833] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-06-06 10:27:48,833] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:48,833] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:49,119] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:49,122] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 79 ms (kafka.log.Log)
[2019-06-06 10:27:49,122] INFO Created log for partition __consumer_offsets-26 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:49,123] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-06-06 10:27:49,123] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:49,123] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:49,424] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:49,427] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 198 ms (kafka.log.Log)
[2019-06-06 10:27:49,430] INFO Created log for partition streams-wordcount-output-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:49,430] INFO [Partition streams-wordcount-output-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-output-0 (kafka.cluster.Partition)
[2019-06-06 10:27:49,430] INFO Replica loaded for partition streams-wordcount-output-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:49,431] INFO [Partition streams-wordcount-output-0 broker=0] streams-wordcount-output-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:49,653] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:49,656] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 29 ms (kafka.log.Log)
[2019-06-06 10:27:49,656] INFO Created log for partition __consumer_offsets-7 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:49,657] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-06-06 10:27:49,657] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:49,657] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:49,964] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:49,967] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 80 ms (kafka.log.Log)
[2019-06-06 10:27:49,968] INFO Created log for partition __consumer_offsets-42 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:49,968] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-06-06 10:27:49,968] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:49,969] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:50,487] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:50,491] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 281 ms (kafka.log.Log)
[2019-06-06 10:27:50,491] INFO Created log for partition __consumer_offsets-4 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:50,492] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-06-06 10:27:50,492] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:50,492] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:50,678] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:50,680] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:50,680] INFO Created log for partition __consumer_offsets-23 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:50,681] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-06-06 10:27:50,681] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:50,681] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:50,893] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:50,897] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-06-06 10:27:50,898] INFO Created log for partition __consumer_offsets-1 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:50,898] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-06-06 10:27:50,898] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:50,899] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:51,080] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:51,083] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[2019-06-06 10:27:51,084] INFO Created log for partition __consumer_offsets-20 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:51,085] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-06-06 10:27:51,085] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:51,085] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:51,304] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:51,308] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-06-06 10:27:51,308] INFO Created log for partition __consumer_offsets-39 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:51,309] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-06-06 10:27:51,309] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:51,309] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:51,430] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:51,432] INFO [Log partition=distributed-video1-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:51,432] INFO Created log for partition distributed-video1-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:51,433] INFO [Partition distributed-video1-0 broker=0] No checkpointed highwatermark is found for partition distributed-video1-0 (kafka.cluster.Partition)
[2019-06-06 10:27:51,433] INFO Replica loaded for partition distributed-video1-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:51,433] INFO [Partition distributed-video1-0 broker=0] distributed-video1-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:51,666] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:51,669] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 10:27:51,670] INFO Created log for partition __consumer_offsets-17 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:51,670] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-06-06 10:27:51,671] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:51,671] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:51,903] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:51,906] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 62 ms (kafka.log.Log)
[2019-06-06 10:27:51,906] INFO Created log for partition __consumer_offsets-36 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:51,907] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-06-06 10:27:51,907] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:51,907] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:52,103] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:52,105] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-06-06 10:27:52,105] INFO Created log for partition __consumer_offsets-14 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:52,106] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-06-06 10:27:52,106] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:52,106] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:52,314] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:52,318] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[2019-06-06 10:27:52,319] INFO Created log for partition __consumer_offsets-33 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:52,319] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-06-06 10:27:52,319] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:52,319] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:52,553] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:52,556] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-06-06 10:27:52,556] INFO Created log for partition __consumer_offsets-49 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:52,557] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-06-06 10:27:52,557] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:52,557] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:52,874] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:52,876] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 121 ms (kafka.log.Log)
[2019-06-06 10:27:52,877] INFO Created log for partition __consumer_offsets-11 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:52,877] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-06-06 10:27:52,878] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:52,878] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:53,152] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:53,154] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-06-06 10:27:53,155] INFO Created log for partition __consumer_offsets-30 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:53,155] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-06-06 10:27:53,155] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:53,156] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:53,284] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:53,286] INFO [Log partition=test-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:53,325] INFO Created log for partition test-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:53,326] INFO [Partition test-0 broker=0] No checkpointed highwatermark is found for partition test-0 (kafka.cluster.Partition)
[2019-06-06 10:27:53,327] INFO Replica loaded for partition test-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:53,327] INFO [Partition test-0 broker=0] test-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:53,602] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:53,605] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-06-06 10:27:53,606] INFO Created log for partition streams-plaintext-input-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:53,606] INFO [Partition streams-plaintext-input-0 broker=0] No checkpointed highwatermark is found for partition streams-plaintext-input-0 (kafka.cluster.Partition)
[2019-06-06 10:27:53,606] INFO Replica loaded for partition streams-plaintext-input-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:53,606] INFO [Partition streams-plaintext-input-0 broker=0] streams-plaintext-input-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:53,820] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:53,823] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[2019-06-06 10:27:53,824] INFO Created log for partition __consumer_offsets-46 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:53,825] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-06-06 10:27:53,825] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:53,825] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,055] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,057] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[2019-06-06 10:27:54,058] INFO Created log for partition __consumer_offsets-27 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,058] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-06-06 10:27:54,059] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,059] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,247] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,249] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 72 ms (kafka.log.Log)
[2019-06-06 10:27:54,250] INFO Created log for partition __consumer_offsets-8 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,251] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-06-06 10:27:54,251] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,251] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,336] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,339] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:54,339] INFO Created log for partition __consumer_offsets-24 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,340] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-06-06 10:27:54,340] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,340] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,436] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,438] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:54,439] INFO Created log for partition __consumer_offsets-43 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,439] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-06-06 10:27:54,440] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,440] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,524] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,527] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:54,528] INFO Created log for partition __consumer_offsets-5 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,529] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-06-06 10:27:54,529] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,529] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,647] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,649] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[2019-06-06 10:27:54,650] INFO Created log for partition __consumer_offsets-21 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,650] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-06-06 10:27:54,650] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,650] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,745] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,748] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 10:27:54,749] INFO Created log for partition __consumer_offsets-2 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,749] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-06-06 10:27:54,749] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,749] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:54,970] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:54,973] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:54,973] INFO Created log for partition __consumer_offsets-40 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:54,974] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-06-06 10:27:54,974] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:54,974] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,227] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,230] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-06-06 10:27:55,230] INFO Created log for partition __consumer_offsets-37 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,231] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-06-06 10:27:55,231] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,231] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,473] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,476] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 10:27:55,476] INFO Created log for partition __consumer_offsets-18 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,477] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-06-06 10:27:55,477] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,477] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,575] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,578] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-06-06 10:27:55,578] INFO Created log for partition __consumer_offsets-34 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,578] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-06-06 10:27:55,579] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,579] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,680] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,683] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:55,684] INFO Created log for partition __consumer_offsets-15 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,684] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-06-06 10:27:55,684] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,684] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,791] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,793] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:55,794] INFO Created log for partition __consumer_offsets-12 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,795] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-06-06 10:27:55,795] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,795] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:55,891] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:55,893] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:55,894] INFO Created log for partition __consumer_offsets-31 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:55,894] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-06-06 10:27:55,895] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:55,895] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,003] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,006] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:56,006] INFO Created log for partition __consumer_offsets-9 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,007] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-06-06 10:27:56,007] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,007] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,168] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,171] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-06-06 10:27:56,171] INFO Created log for partition __consumer_offsets-47 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,171] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-06-06 10:27:56,172] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,172] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,258] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,260] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:56,260] INFO Created log for partition __consumer_offsets-19 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,260] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-06-06 10:27:56,260] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,262] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,599] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,605] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 162 ms (kafka.log.Log)
[2019-06-06 10:27:56,608] INFO Created log for partition __consumer_offsets-28 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,611] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-06-06 10:27:56,611] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,611] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,787] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,789] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2019-06-06 10:27:56,789] INFO Created log for partition __consumer_offsets-38 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,790] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-06-06 10:27:56,790] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,790] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:56,950] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:56,952] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-06-06 10:27:56,953] INFO Created log for partition __consumer_offsets-35 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:56,953] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-06-06 10:27:56,953] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:56,953] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:57,161] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:57,163] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-06-06 10:27:57,164] INFO Created log for partition __consumer_offsets-6 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:57,164] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-06-06 10:27:57,164] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:57,164] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:57,376] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:57,379] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-06-06 10:27:57,386] INFO Created log for partition __consumer_offsets-44 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:57,388] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-06-06 10:27:57,389] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:57,389] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:57,708] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:57,710] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-06-06 10:27:57,711] INFO Created log for partition __consumer_offsets-25 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:57,712] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-06-06 10:27:57,713] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:57,713] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:57,919] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:57,922] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 67 ms (kafka.log.Log)
[2019-06-06 10:27:57,923] INFO Created log for partition __consumer_offsets-16 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:57,923] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-06-06 10:27:57,923] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:57,924] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,181] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:58,183] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 10:27:58,184] INFO Created log for partition __consumer_offsets-22 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:58,184] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-06-06 10:27:58,184] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:58,184] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,292] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:58,294] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 10:27:58,295] INFO Created log for partition __consumer_offsets-41 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:58,295] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-06-06 10:27:58,295] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:58,296] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,450] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:58,453] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 31 ms (kafka.log.Log)
[2019-06-06 10:27:58,453] INFO Created log for partition __consumer_offsets-32 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:58,454] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-06-06 10:27:58,454] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:58,454] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,582] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:58,586] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[2019-06-06 10:27:58,587] INFO Created log for partition __consumer_offsets-3 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:58,587] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-06-06 10:27:58,587] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:58,587] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,750] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:27:58,752] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 42 ms (kafka.log.Log)
[2019-06-06 10:27:58,753] INFO Created log for partition __consumer_offsets-13 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:27:58,753] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-06-06 10:27:58,753] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:27:58,754] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:27:58,908] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,911] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,911] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,911] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,911] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,911] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,913] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,915] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,915] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,915] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,915] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,915] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:58,917] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,089] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 178 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,090] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,091] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,091] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,091] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,091] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,091] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,092] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,092] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,092] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,092] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,092] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,093] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,093] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,093] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,094] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,095] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,096] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,096] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,096] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,096] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,096] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,097] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,098] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,098] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,099] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,100] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,100] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,100] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,101] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,101] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,101] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,101] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:27:59,101] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:37:45,816] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:45:55,790] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member streams-wordcount-4f99394e-8f68-4e7c-aea1-b05b0d100cb6-StreamThread-1-consumer-6e87dca1-360d-4d56-b86b-23ca8d1ac647) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:45:56,223] INFO [GroupCoordinator 0]: Stabilized group streams-wordcount generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:45:57,553] INFO Creating topic streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition with configuration {segment.bytes=52428800, retention.ms=9223372036854775807, segment.index.bytes=52428800, segment.ms=600000, cleanup.policy=delete} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 10:45:57,823] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:setData cxid:0xba zxid:0x1d8 txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:45:59,950] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 10:46:00,284] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:46:00,287] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 316 ms (kafka.log.Log)
[2019-06-06 10:46:00,338] INFO Created log for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 600000, segment.bytes -> 52428800, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 52428800, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:46:00,351] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 (kafka.cluster.Partition)
[2019-06-06 10:46:00,351] INFO Replica loaded for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:46:00,352] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 broker=0] streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:46:00,610] INFO Creating topic streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 10:46:00,612] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:setData cxid:0xc4 zxid:0x1de txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 10:46:01,115] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 10:46:01,164] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 10:46:01,183] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 10:46:01,238] INFO Created log for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 10:46:01,241] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 (kafka.cluster.Partition)
[2019-06-06 10:46:01,242] INFO Replica loaded for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 10:46:01,242] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 broker=0] streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 10:46:01,685] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-wordcount for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:46:15,708] INFO [GroupCoordinator 0]: Member streams-wordcount-4f99394e-8f68-4e7c-aea1-b05b0d100cb6-StreamThread-1-consumer-6e87dca1-360d-4d56-b86b-23ca8d1ac647 in group streams-wordcount has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:46:15,711] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member streams-wordcount-4f99394e-8f68-4e7c-aea1-b05b0d100cb6-StreamThread-1-consumer-6e87dca1-360d-4d56-b86b-23ca8d1ac647 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:46:15,713] INFO [GroupCoordinator 0]: Group streams-wordcount with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:47:45,819] INFO [GroupMetadataManager brokerId=0] Group streams-wordcount transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:47:45,889] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 72 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:50:58,607] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 0 (__consumer_offsets-8) (reason: Adding new member streams-pipe-be505177-ee6f-42fd-955c-f14a79236f51-StreamThread-1-consumer-8440d43f-8e6c-4f30-b0d0-1abadee530f8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:50:58,616] INFO [GroupCoordinator 0]: Stabilized group streams-pipe generation 1 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:50:58,647] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-pipe for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:51:22,441] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 0 (__consumer_offsets-24) (reason: Adding new member streams-linesplit-ed190143-0fe2-4eb7-9898-728bf28922b1-StreamThread-1-consumer-ae6f6410-893c-40af-9613-2ec67bc8ebb1) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:51:22,442] INFO [GroupCoordinator 0]: Stabilized group streams-linesplit generation 1 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:51:22,452] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-linesplit for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:52:50,487] INFO [GroupCoordinator 0]: Member streams-linesplit-ed190143-0fe2-4eb7-9898-728bf28922b1-StreamThread-1-consumer-ae6f6410-893c-40af-9613-2ec67bc8ebb1 in group streams-linesplit has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:52:50,487] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 1 (__consumer_offsets-24) (reason: removing member streams-linesplit-ed190143-0fe2-4eb7-9898-728bf28922b1-StreamThread-1-consumer-ae6f6410-893c-40af-9613-2ec67bc8ebb1 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:52:50,488] INFO [GroupCoordinator 0]: Group streams-linesplit with generation 2 is now empty (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:53:11,772] INFO [GroupCoordinator 0]: Member streams-pipe-be505177-ee6f-42fd-955c-f14a79236f51-StreamThread-1-consumer-8440d43f-8e6c-4f30-b0d0-1abadee530f8 in group streams-pipe has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:53:11,772] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 1 (__consumer_offsets-8) (reason: removing member streams-pipe-be505177-ee6f-42fd-955c-f14a79236f51-StreamThread-1-consumer-8440d43f-8e6c-4f30-b0d0-1abadee530f8 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:53:11,772] INFO [GroupCoordinator 0]: Group streams-pipe with generation 2 is now empty (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 10:57:45,837] INFO [GroupMetadataManager brokerId=0] Group streams-linesplit transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:57:45,838] INFO [GroupMetadataManager brokerId=0] Group streams-pipe transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 10:57:45,838] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:00:23,588] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 0 (__consumer_offsets-8) (reason: Adding new member streams-pipe-8f9262dd-cd02-430d-a22b-82854a34cf1a-StreamThread-1-consumer-02ed94f4-9e4e-4cd6-aa28-560fa2b05e58) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:00:24,311] INFO [GroupCoordinator 0]: Stabilized group streams-pipe generation 1 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:00:24,332] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-pipe for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:17,021] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 0 (__consumer_offsets-24) (reason: Adding new member streams-linesplit-64883d60-43b4-4d15-8d66-7816e1099e48-StreamThread-1-consumer-313e84c7-2aae-43f7-9de0-55c73ff51ce7) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:17,022] INFO [GroupCoordinator 0]: Stabilized group streams-linesplit generation 1 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:17,030] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-linesplit for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:31,352] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member streams-wordcount-504e0211-0775-482e-9f53-10a4bee401f8-StreamThread-1-consumer-392ceba3-a044-43b8-95b0-1c86636b63e1) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:31,353] INFO [GroupCoordinator 0]: Stabilized group streams-wordcount generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:01:31,378] INFO Creating topic streams-wordcount-counts-store-repartition with configuration {segment.bytes=52428800, retention.ms=9223372036854775807, segment.index.bytes=52428800, segment.ms=600000, cleanup.policy=delete} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:01:31,380] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:setData cxid:0xce zxid:0x1e4 txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-counts-store-repartition Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-counts-store-repartition (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:01:32,540] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-counts-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:01:32,989] ERROR Error while creating log for streams-wordcount-counts-store-repartition-0 in dir C:\tmp\kafka-logs (kafka.server.LogDirFailureChannel)
java.io.IOException: Map failed
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:940)
	at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:126)
	at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:53)
	at kafka.log.LogSegment$.open(LogSegment.scala:632)
	at kafka.log.Log.loadSegments(Log.scala:575)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.$anonfun$getOrCreateLog$1(LogManager.scala:716)
	at scala.Option.getOrElse(Option.scala:138)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition.$anonfun$getOrCreateReplica$1(Partition.scala:202)
	at kafka.utils.Pool$$anon$1.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:198)
	at kafka.cluster.Partition.$anonfun$makeLeader$3(Partition.scala:376)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at kafka.cluster.Partition.$anonfun$makeLeader$1(Partition.scala:376)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:259)
	at kafka.cluster.Partition.makeLeader(Partition.scala:370)
	at kafka.server.ReplicaManager.$anonfun$makeLeaders$5(ReplicaManager.scala:1188)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1186)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1098)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:195)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:112)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Map failed
	at sun.nio.ch.FileChannelImpl.map0(Native Method)
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:937)
	... 41 more
[2019-06-06 11:01:33,004] ERROR [ReplicaManager broker=0] Error while making broker the leader for partition Topic: streams-wordcount-counts-store-repartition; Partition: 0; Leader: None; AllReplicas: ; InSyncReplicas:  in dir None (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.KafkaStorageException: Error while creating log for streams-wordcount-counts-store-repartition-0 in dir C:\tmp\kafka-logs
Caused by: java.io.IOException: Map failed
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:940)
	at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:126)
	at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:53)
	at kafka.log.LogSegment$.open(LogSegment.scala:632)
	at kafka.log.Log.loadSegments(Log.scala:575)
	at kafka.log.Log.<init>(Log.scala:292)
	at kafka.log.Log$.apply(Log.scala:2158)
	at kafka.log.LogManager.$anonfun$getOrCreateLog$1(LogManager.scala:716)
	at scala.Option.getOrElse(Option.scala:138)
	at kafka.log.LogManager.getOrCreateLog(LogManager.scala:674)
	at kafka.cluster.Partition.$anonfun$getOrCreateReplica$1(Partition.scala:202)
	at kafka.utils.Pool$$anon$1.apply(Pool.scala:61)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at kafka.utils.Pool.getAndMaybePut(Pool.scala:60)
	at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:198)
	at kafka.cluster.Partition.$anonfun$makeLeader$3(Partition.scala:376)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:237)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at kafka.cluster.Partition.$anonfun$makeLeader$1(Partition.scala:376)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.utils.CoreUtils$.inWriteLock(CoreUtils.scala:259)
	at kafka.cluster.Partition.makeLeader(Partition.scala:370)
	at kafka.server.ReplicaManager.$anonfun$makeLeaders$5(ReplicaManager.scala:1188)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
	at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:1186)
	at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:1098)
	at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:195)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:112)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Map failed
	at sun.nio.ch.FileChannelImpl.map0(Native Method)
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:937)
	... 41 more
[2019-06-06 11:01:33,006] INFO [ReplicaManager broker=0] Stopping serving replicas in dir C:\tmp\kafka-logs (kafka.server.ReplicaManager)
[2019-06-06 11:01:33,014] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-plaintext-input-0, __consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, test-0, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, distributed-video1-0, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, javainuse-topic-0, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, streams-wordcount-output-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:01:33,033] INFO Creating topic streams-wordcount-counts-store-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:01:33,038] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(streams-plaintext-input-0, __consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, test-0, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, distributed-video1-0, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, javainuse-topic-0, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, streams-wordcount-output-0, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 11:01:33,039] INFO Got user-level KeeperException when processing sessionid:0x1000c82ec300000 type:setData cxid:0xda zxid:0x1ea txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-counts-store-changelog Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-counts-store-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:01:33,837] INFO [ReplicaManager broker=0] Broker 0 stopped fetcher for partitions streams-plaintext-input-0,__consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,test-0,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,distributed-video1-0,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,javainuse-topic-0,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,streams-wordcount-output-0,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 and stopped moving logs for partitions  because they are in the failed log directory C:\tmp\kafka-logs. (kafka.server.ReplicaManager)
[2019-06-06 11:01:33,854] INFO Stopping serving logs in dir C:\tmp\kafka-logs (kafka.log.LogManager)
[2019-06-06 11:01:33,937] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:01:33,986] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set() (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 11:01:34,002] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-counts-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:01:34,002] INFO [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-counts-store-repartition-0) (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 11:01:34,251] ERROR Shutdown broker because all log dirs in C:\tmp\kafka-logs have failed (kafka.log.LogManager)
[2019-06-06 11:01:34,514] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-counts-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:01:35,680] WARN Exception causing close of session 0x1000c82ec300000: 远程主机强迫关闭了一个现有的连接。 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 11:01:36,208] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:49531 which had sessionid 0x1000c82ec300000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 11:01:40,636] INFO Expiring session 0x1000c82ec300000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:01:40,636] INFO Processed session termination for sessionid: 0x1000c82ec300000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:41,654] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 11:02:41,704] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:02:41,704] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:02:41,704] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:02:41,705] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-06-06 11:02:41,742] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 11:02:41,742] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-06-06 11:02:41,813] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,814] INFO Server environment:host.name=xqy-HP (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,815] INFO Server environment:java.version=1.8.0_111 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,815] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,816] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,816] INFO Server environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,819] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:os.arch=x86 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,820] INFO Server environment:user.name=xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,821] INFO Server environment:user.home=C:\Users\xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,821] INFO Server environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,837] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,837] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,838] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:41,919] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-06-06 11:02:41,935] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 11:02:50,589] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 11:02:52,556] INFO starting (kafka.server.KafkaServer)
[2019-06-06 11:02:52,560] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 11:02:52,801] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:02:52,832] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,832] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,833] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,834] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,836] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,837] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,839] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,840] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,840] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,841] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,841] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,841] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,842] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,842] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,842] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,845] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@130745 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:02:52,881] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:02:52,882] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:02:52,883] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:02:52,883] INFO Accepted socket connection from /127.0.0.1:53124 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 11:02:52,921] INFO Client attempting to establish new session at /127.0.0.1:53124 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:52,926] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-06-06 11:02:53,081] INFO Established session 0x1000ca320eb0000 with negotiated timeout 6000 for client /127.0.0.1:53124 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:02:53,083] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000ca320eb0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:02:53,086] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:02:53,471] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:53,684] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:53,818] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:54,852] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:55,007] INFO Cluster ID = LW9rTJf-QKWbIMgjpFjing (kafka.server.KafkaServer)
[2019-06-06 11:02:55,038] WARN No meta.properties file under dir C:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-06-06 11:02:55,189] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 11:02:55,201] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 11:02:55,370] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:02:55,371] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:02:55,370] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:02:55,603] INFO Log directory C:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-06-06 11:02:55,667] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 11:02:55,677] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-06-06 11:02:55,722] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-06-06 11:02:55,725] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-06-06 11:02:56,613] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[2019-06-06 11:02:56,917] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-06-06 11:02:56,920] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-06-06 11:02:57,278] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:57,280] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:57,281] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:57,281] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:57,306] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 11:02:57,390] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-06-06 11:02:57,559] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1559790177489,1559790177489,1,0,0,72071488809402368,182,0,24
 (kafka.zk.KafkaZkClient)
[2019-06-06 11:02:57,561] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(xqy-HP,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2019-06-06 11:02:57,562] WARN No meta.properties file under dir C:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-06-06 11:02:58,090] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:58,095] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:58,096] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:02:58,229] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-06-06 11:02:58,320] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:02:58,363] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:02:58,379] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 16 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:02:58,512] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-06-06 11:02:58,625] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:02:58,628] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:02:58,628] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 11:02:58,747] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:multi cxid:0x32 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:58,852] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 11:02:58,861] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-06-06 11:02:58,901] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 11:02:58,903] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 11:02:58,905] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-06-06 11:02:59,071] INFO Creating topic streams-wordcount-counts-store-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:02:59,073] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:02:59,074] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:setData cxid:0x46 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-counts-store-changelog Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-counts-store-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:59,072] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(0), 32 -> ArrayBuffer(0), 41 -> ArrayBuffer(0), 17 -> ArrayBuffer(0), 8 -> ArrayBuffer(0), 35 -> ArrayBuffer(0), 44 -> ArrayBuffer(0), 26 -> ArrayBuffer(0), 11 -> ArrayBuffer(0), 29 -> ArrayBuffer(0), 38 -> ArrayBuffer(0), 47 -> ArrayBuffer(0), 20 -> ArrayBuffer(0), 2 -> ArrayBuffer(0), 5 -> ArrayBuffer(0), 14 -> ArrayBuffer(0), 46 -> ArrayBuffer(0), 49 -> ArrayBuffer(0), 40 -> ArrayBuffer(0), 13 -> ArrayBuffer(0), 4 -> ArrayBuffer(0), 22 -> ArrayBuffer(0), 31 -> ArrayBuffer(0), 16 -> ArrayBuffer(0), 7 -> ArrayBuffer(0), 43 -> ArrayBuffer(0), 25 -> ArrayBuffer(0), 34 -> ArrayBuffer(0), 10 -> ArrayBuffer(0), 37 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 19 -> ArrayBuffer(0), 28 -> ArrayBuffer(0), 45 -> ArrayBuffer(0), 27 -> ArrayBuffer(0), 36 -> ArrayBuffer(0), 18 -> ArrayBuffer(0), 9 -> ArrayBuffer(0), 21 -> ArrayBuffer(0), 48 -> ArrayBuffer(0), 3 -> ArrayBuffer(0), 12 -> ArrayBuffer(0), 30 -> ArrayBuffer(0), 39 -> ArrayBuffer(0), 15 -> ArrayBuffer(0), 42 -> ArrayBuffer(0), 24 -> ArrayBuffer(0), 6 -> ArrayBuffer(0), 33 -> ArrayBuffer(0), 0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:02:59,074] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:setData cxid:0x47 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:59,076] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:setData cxid:0x48 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:59,187] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x4b zxid:0x22 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:02:59,382] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-06-06 11:02:59,385] INFO Got user-level KeeperException when processing sessionid:0x1000ca320eb0000 type:create cxid:0x52 zxid:0x26 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /brokers/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:03:00,811] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, streams-wordcount-counts-store-changelog-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:03:01,320] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:01,361] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 475 ms (kafka.log.Log)
[2019-06-06 11:03:01,364] INFO Created log for partition __consumer_offsets-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:01,366] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-06-06 11:03:01,369] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:01,374] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:01,672] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:01,675] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 61 ms (kafka.log.Log)
[2019-06-06 11:03:01,675] INFO Created log for partition __consumer_offsets-29 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:01,676] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-06-06 11:03:01,676] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:01,676] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:01,996] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:01,998] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 110 ms (kafka.log.Log)
[2019-06-06 11:03:01,999] INFO Created log for partition __consumer_offsets-48 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:01,999] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-06-06 11:03:02,000] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:02,000] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:02,292] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:02,296] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 148 ms (kafka.log.Log)
[2019-06-06 11:03:02,297] INFO Created log for partition __consumer_offsets-10 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:02,297] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-06-06 11:03:02,297] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:02,298] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:02,555] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:02,557] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[2019-06-06 11:03:02,558] INFO Created log for partition __consumer_offsets-45 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:02,558] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-06-06 11:03:02,559] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:02,559] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:02,842] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:02,845] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-06-06 11:03:02,846] INFO Created log for partition __consumer_offsets-26 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:02,847] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-06-06 11:03:02,847] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:02,847] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,045] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,047] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:03:03,048] INFO Created log for partition __consumer_offsets-7 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,048] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-06-06 11:03:03,048] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,049] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,235] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,238] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 29 ms (kafka.log.Log)
[2019-06-06 11:03:03,238] INFO Created log for partition __consumer_offsets-42 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,238] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-06-06 11:03:03,239] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,239] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,423] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,426] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 11:03:03,428] INFO Created log for partition __consumer_offsets-4 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,428] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-06-06 11:03:03,428] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,428] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,535] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,538] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:03,538] INFO Created log for partition __consumer_offsets-23 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,539] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-06-06 11:03:03,539] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,539] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,679] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,681] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:03,682] INFO Created log for partition __consumer_offsets-1 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,682] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,683] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,683] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:03,824] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:03,827] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:03,828] INFO Created log for partition __consumer_offsets-20 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:03,828] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-06-06 11:03:03,828] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:03,829] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,015] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,017] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-06-06 11:03:04,018] INFO Created log for partition __consumer_offsets-39 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,018] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-06-06 11:03:04,019] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,019] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,149] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,151] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[2019-06-06 11:03:04,152] INFO Created log for partition __consumer_offsets-17 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,152] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-06-06 11:03:04,152] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,153] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,268] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,271] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:04,272] INFO Created log for partition __consumer_offsets-36 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,272] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-06-06 11:03:04,272] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,272] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,506] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,510] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 91 ms (kafka.log.Log)
[2019-06-06 11:03:04,511] INFO Created log for partition __consumer_offsets-14 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,511] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-06-06 11:03:04,511] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,512] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,593] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,596] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:04,596] INFO Created log for partition __consumer_offsets-33 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,597] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-06-06 11:03:04,597] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,597] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,750] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,752] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:04,753] INFO Created log for partition __consumer_offsets-49 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,753] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-06-06 11:03:04,753] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,753] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:04,891] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:04,894] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-06-06 11:03:04,894] INFO Created log for partition __consumer_offsets-11 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:04,895] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-06-06 11:03:04,895] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:04,895] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,117] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,120] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[2019-06-06 11:03:05,121] INFO Created log for partition __consumer_offsets-30 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,128] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-06-06 11:03:05,128] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,128] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,302] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,304] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:05,304] INFO Created log for partition __consumer_offsets-46 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,305] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-06-06 11:03:05,305] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,305] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,457] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,460] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:05,461] INFO Created log for partition __consumer_offsets-27 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,461] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-06-06 11:03:05,461] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,462] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,581] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,584] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:05,584] INFO Created log for partition __consumer_offsets-8 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,585] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-06-06 11:03:05,585] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,585] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,700] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,704] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:05,704] INFO Created log for partition __consumer_offsets-24 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,705] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-06-06 11:03:05,705] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,705] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,834] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,837] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:05,837] INFO Created log for partition __consumer_offsets-43 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,837] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-06-06 11:03:05,838] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,838] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:05,995] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:05,997] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-06-06 11:03:05,998] INFO Created log for partition __consumer_offsets-5 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:05,998] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-06-06 11:03:05,998] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:05,998] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:06,172] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:06,175] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 33 ms (kafka.log.Log)
[2019-06-06 11:03:06,176] INFO Created log for partition __consumer_offsets-21 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:06,176] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-06-06 11:03:06,176] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:06,176] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:06,429] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:06,432] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 57 ms (kafka.log.Log)
[2019-06-06 11:03:06,432] INFO Created log for partition __consumer_offsets-2 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:06,433] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-06-06 11:03:06,433] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:06,433] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:06,639] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:06,643] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-06-06 11:03:06,644] INFO Created log for partition __consumer_offsets-40 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:06,644] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-06-06 11:03:06,644] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:06,644] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:06,785] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:06,788] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-06-06 11:03:06,789] INFO Created log for partition __consumer_offsets-37 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:06,789] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-06-06 11:03:06,790] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:06,790] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:06,985] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:06,990] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-06-06 11:03:06,991] INFO Created log for partition __consumer_offsets-18 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:06,992] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-06-06 11:03:06,992] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:06,992] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:07,174] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:07,180] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 70 ms (kafka.log.Log)
[2019-06-06 11:03:07,181] INFO Created log for partition __consumer_offsets-34 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:07,182] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-06-06 11:03:07,182] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:07,182] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:07,391] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:07,393] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 74 ms (kafka.log.Log)
[2019-06-06 11:03:07,394] INFO Created log for partition __consumer_offsets-15 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:07,394] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-06-06 11:03:07,394] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:07,394] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:07,576] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:07,579] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2019-06-06 11:03:07,579] INFO Created log for partition __consumer_offsets-12 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:07,580] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-06-06 11:03:07,580] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:07,580] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:07,802] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:07,804] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 11:03:07,805] INFO Created log for partition __consumer_offsets-31 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:07,805] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-06-06 11:03:07,805] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:07,805] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:08,052] INFO [Log partition=streams-wordcount-counts-store-changelog-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:08,054] INFO [Log partition=streams-wordcount-counts-store-changelog-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[2019-06-06 11:03:08,055] INFO Created log for partition streams-wordcount-counts-store-changelog-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:08,055] INFO [Partition streams-wordcount-counts-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-counts-store-changelog-0 (kafka.cluster.Partition)
[2019-06-06 11:03:08,055] INFO Replica loaded for partition streams-wordcount-counts-store-changelog-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:08,055] INFO [Partition streams-wordcount-counts-store-changelog-0 broker=0] streams-wordcount-counts-store-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:08,335] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:08,341] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 78 ms (kafka.log.Log)
[2019-06-06 11:03:08,342] INFO Created log for partition __consumer_offsets-9 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:08,342] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-06-06 11:03:08,342] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:08,343] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:08,539] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:08,542] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 42 ms (kafka.log.Log)
[2019-06-06 11:03:08,543] INFO Created log for partition __consumer_offsets-47 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:08,543] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-06-06 11:03:08,543] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:08,543] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:08,766] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:08,769] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-06-06 11:03:08,769] INFO Created log for partition __consumer_offsets-19 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:08,770] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-06-06 11:03:08,770] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:08,770] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:08,917] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:08,920] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 30 ms (kafka.log.Log)
[2019-06-06 11:03:08,920] INFO Created log for partition __consumer_offsets-28 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:08,921] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-06-06 11:03:08,921] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:08,921] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,058] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,061] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:09,062] INFO Created log for partition __consumer_offsets-38 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,063] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-06-06 11:03:09,063] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,063] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,219] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,222] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-06-06 11:03:09,222] INFO Created log for partition __consumer_offsets-35 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,223] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-06-06 11:03:09,223] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,223] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,357] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,359] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[2019-06-06 11:03:09,360] INFO Created log for partition __consumer_offsets-44 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,360] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-06-06 11:03:09,360] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,361] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,426] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,429] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:09,430] INFO Created log for partition __consumer_offsets-6 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,432] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-06-06 11:03:09,432] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,432] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,534] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,537] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:09,539] INFO Created log for partition __consumer_offsets-25 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,539] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-06-06 11:03:09,539] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,539] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,645] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,648] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:09,649] INFO Created log for partition __consumer_offsets-16 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,649] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-06-06 11:03:09,649] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,649] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,781] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,784] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-06-06 11:03:09,785] INFO Created log for partition __consumer_offsets-22 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,786] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-06-06 11:03:09,786] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,787] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:09,928] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:09,931] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:09,932] INFO Created log for partition __consumer_offsets-41 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:09,932] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-06-06 11:03:09,932] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:09,932] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:10,062] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:10,064] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 42 ms (kafka.log.Log)
[2019-06-06 11:03:10,064] INFO Created log for partition __consumer_offsets-32 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:10,065] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-06-06 11:03:10,065] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:10,065] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:10,191] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:10,194] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-06-06 11:03:10,195] INFO Created log for partition __consumer_offsets-3 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:10,196] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-06-06 11:03:10,196] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:10,196] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:10,293] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:03:10,295] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 11:03:10,296] INFO Created log for partition __consumer_offsets-13 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:03:10,296] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-06-06 11:03:10,296] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:03:10,296] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:03:10,416] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,417] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,419] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,422] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,423] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,424] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,425] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,426] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,431] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,432] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,618] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 201 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,642] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,642] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,642] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,644] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,645] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,646] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,646] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,646] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,646] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,647] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,648] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,649] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,650] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,650] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,655] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,656] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,657] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,658] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,658] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,658] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:03:10,726] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 0 (__consumer_offsets-24) (reason: Adding new member streams-linesplit-64883d60-43b4-4d15-8d66-7816e1099e48-StreamThread-1-consumer-d97e431c-7a3d-4274-b201-183bc281154a) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:10,726] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 0 (__consumer_offsets-8) (reason: Adding new member streams-pipe-8f9262dd-cd02-430d-a22b-82854a34cf1a-StreamThread-1-consumer-1491176f-11a9-4ea5-88d4-b76df812abd3) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:10,734] INFO [GroupCoordinator 0]: Stabilized group streams-linesplit generation 1 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:10,739] INFO [GroupCoordinator 0]: Stabilized group streams-pipe generation 1 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:10,747] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-pipe for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:10,750] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-linesplit for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,854] INFO [GroupCoordinator 0]: Member streams-pipe-8f9262dd-cd02-430d-a22b-82854a34cf1a-StreamThread-1-consumer-1491176f-11a9-4ea5-88d4-b76df812abd3 in group streams-pipe has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,858] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 1 (__consumer_offsets-8) (reason: removing member streams-pipe-8f9262dd-cd02-430d-a22b-82854a34cf1a-StreamThread-1-consumer-1491176f-11a9-4ea5-88d4-b76df812abd3 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,860] INFO [GroupCoordinator 0]: Group streams-pipe with generation 2 is now empty (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,865] INFO [GroupCoordinator 0]: Member streams-linesplit-64883d60-43b4-4d15-8d66-7816e1099e48-StreamThread-1-consumer-d97e431c-7a3d-4274-b201-183bc281154a in group streams-linesplit has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,866] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 1 (__consumer_offsets-24) (reason: removing member streams-linesplit-64883d60-43b4-4d15-8d66-7816e1099e48-StreamThread-1-consumer-d97e431c-7a3d-4274-b201-183bc281154a on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:20,866] INFO [GroupCoordinator 0]: Group streams-linesplit with generation 2 is now empty (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:40,609] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 2 (__consumer_offsets-8) (reason: Adding new member streams-pipe-76c2065f-decf-4f99-8a2e-852e7a89f5a7-StreamThread-1-consumer-5c5a3487-de7a-48b0-856f-f8f5fbbff3b1) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:40,610] INFO [GroupCoordinator 0]: Stabilized group streams-pipe generation 3 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:40,615] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-pipe for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:50,618] INFO [GroupCoordinator 0]: Member streams-pipe-76c2065f-decf-4f99-8a2e-852e7a89f5a7-StreamThread-1-consumer-5c5a3487-de7a-48b0-856f-f8f5fbbff3b1 in group streams-pipe has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:50,619] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-pipe in state PreparingRebalance with old generation 3 (__consumer_offsets-8) (reason: removing member streams-pipe-76c2065f-decf-4f99-8a2e-852e7a89f5a7-StreamThread-1-consumer-5c5a3487-de7a-48b0-856f-f8f5fbbff3b1 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:50,619] INFO [GroupCoordinator 0]: Group streams-pipe with generation 4 is now empty (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:52,215] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 2 (__consumer_offsets-24) (reason: Adding new member streams-linesplit-aec9f61f-afbe-40bf-ba3c-4b30ed035979-StreamThread-1-consumer-cedd5d76-c151-422d-a85e-4e48168ba5d2) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:52,215] INFO [GroupCoordinator 0]: Stabilized group streams-linesplit generation 3 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:03:52,222] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-linesplit for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:01,368] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member streams-wordcount-71077e15-b9b8-41c0-8e00-b079678161a4-StreamThread-1-consumer-d195176f-2d4b-4f39-872d-bc7df1759857) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:01,369] INFO [GroupCoordinator 0]: Stabilized group streams-wordcount generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:01,375] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-wordcount for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:02,224] INFO [GroupCoordinator 0]: Member streams-linesplit-aec9f61f-afbe-40bf-ba3c-4b30ed035979-StreamThread-1-consumer-cedd5d76-c151-422d-a85e-4e48168ba5d2 in group streams-linesplit has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:02,224] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-linesplit in state PreparingRebalance with old generation 3 (__consumer_offsets-24) (reason: removing member streams-linesplit-aec9f61f-afbe-40bf-ba3c-4b30ed035979-StreamThread-1-consumer-cedd5d76-c151-422d-a85e-4e48168ba5d2 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:02,225] INFO [GroupCoordinator 0]: Group streams-linesplit with generation 4 is now empty (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:11,379] INFO [GroupCoordinator 0]: Member streams-wordcount-71077e15-b9b8-41c0-8e00-b079678161a4-StreamThread-1-consumer-d195176f-2d4b-4f39-872d-bc7df1759857 in group streams-wordcount has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:11,379] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member streams-wordcount-71077e15-b9b8-41c0-8e00-b079678161a4-StreamThread-1-consumer-d195176f-2d4b-4f39-872d-bc7df1759857 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:04:11,380] INFO [GroupCoordinator 0]: Group streams-wordcount with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:12:58,387] INFO [GroupMetadataManager brokerId=0] Group streams-wordcount transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:12:58,394] INFO [GroupMetadataManager brokerId=0] Group streams-linesplit transitioned to Dead in generation 4 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:12:58,395] INFO [GroupMetadataManager brokerId=0] Group streams-pipe transitioned to Dead in generation 4 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:12:58,395] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 32 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:13:48,702] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-06-06 11:13:48,709] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-06-06 11:13:48,798] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2019-06-06 11:13:48,802] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 11:13:48,839] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 11:13:48,839] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 11:13:48,840] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-06-06 11:13:48,849] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-06-06 11:13:48,850] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-06-06 11:13:48,853] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-06-06 11:13:48,855] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-06-06 11:13:48,856] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:48,977] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:48,977] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:48,978] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:13:48,979] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[2019-06-06 11:13:48,980] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-06-06 11:13:48,980] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 11:13:48,980] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 11:13:48,980] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 11:13:48,980] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:13:48,981] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:13:48,981] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,081] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,081] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,082] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,193] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,193] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,195] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:13:49,196] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-06-06 11:13:49,197] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 11:13:49,197] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 11:13:49,197] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 11:13:49,198] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:13:49,218] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:13:49,219] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 11:13:49,220] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 11:13:49,220] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,376] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,376] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,376] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,578] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,578] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,579] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,768] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,768] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,769] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,791] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,791] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:13:49,847] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-06-06 11:13:49,848] INFO Shutting down. (kafka.log.LogManager)
[2019-06-06 11:13:50,148] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2019-06-06 11:13:50,336] INFO [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-06-06 11:13:50,525] INFO [ProducerStateManager partition=__consumer_offsets-24] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2019-06-06 11:13:51,006] INFO Shutdown complete. (kafka.log.LogManager)
[2019-06-06 11:13:51,025] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:13:51,027] INFO Processed session termination for sessionid: 0x1000ca320eb0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:13:51,092] INFO Session: 0x1000ca320eb0000 closed (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:13:51,093] INFO EventThread shut down for session: 0x1000ca320eb0000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:13:51,093] WARN Unable to read additional data from client sessionid 0x1000ca320eb0000, likely client has closed socket (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 11:13:51,095] INFO Closed socket connection for client /127.0.0.1:53124 which had sessionid 0x1000ca320eb0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-06-06 11:13:51,095] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:13:51,096] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:51,728] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:51,728] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:51,728] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,728] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,728] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,729] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,730] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,730] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:13:52,731] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-06-06 11:13:52,756] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-06-06 11:13:52,760] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2019-06-06 11:14:55,301] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 11:14:55,405] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:14:55,406] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:14:55,406] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-06-06 11:14:55,406] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-06-06 11:14:55,473] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-06-06 11:14:55,474] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-06-06 11:14:55,513] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,514] INFO Server environment:host.name=xqy-HP (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,514] INFO Server environment:java.version=1.8.0_111 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,514] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,514] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,514] INFO Server environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.j;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.3.0-SNAPSHOT-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,515] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,516] INFO Server environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,516] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,516] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,517] INFO Server environment:os.arch=x86 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,517] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,517] INFO Server environment:user.name=xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,517] INFO Server environment:user.home=C:\Users\xqy (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,517] INFO Server environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,644] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,644] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,645] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:14:55,907] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-06-06 11:14:55,926] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 11:15:03,904] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-06-06 11:15:05,348] INFO starting (kafka.server.KafkaServer)
[2019-06-06 11:15:05,350] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-06-06 11:15:05,450] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:15:05,457] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,458] INFO Client environment:host.name=xqy-HP (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,458] INFO Client environment:java.version=1.8.0_111 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,458] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,458] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_111\jre (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,458] INFO Client environment:java.class.path=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\activation-1.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\aopalliance-repackaged-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\argparse4j-0.7.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\audience-annotations-0.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\commons-lang3-3.8.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-api-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-basic-auth-extension-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-file-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-json-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-runtime-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\connect-transforms-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\guava-20.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-api-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-locator-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\hk2-utils-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-core-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-databind-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-datatype-jdk8-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-base-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-jaxrs-json-provider-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jackson-module-jaxb-annotations-2.9.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javassist-3.22.0-CR2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.annotation-api-1.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.inject-2.5.0-b42.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.servlet-api-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\javax.ws.rs-api-2.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jaxb-api-2.3.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-client-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-common-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-container-servlet-core-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-hk2-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-media-jaxb-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jersey-server-2.27.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-client-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-continuation-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-http-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-io-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-security-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-server-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlet-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-servlets-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jetty-util-9.4.14.v20181114.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\jopt-simple-5.0.4.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-clients-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-log4j-appender-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.2.0.j;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-examples-2.3.0-SNAPSHOT-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-scala_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-streams-test-utils-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka-tools-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-javadoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-scaladoc.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test-sources.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0-test.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\kafka_2.12-2.2.0.jar.asc;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\log4j-1.2.17.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\lz4-java-1.5.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\maven-artifact-3.6.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\metrics-core-2.2.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\osgi-resource-locator-1.0.1.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\plexus-utils-3.1.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\reflections-0.9.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\rocksdbjni-5.15.10.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-library-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-logging_2.12-3.9.0.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\scala-reflect-2.12.8.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-api-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\slf4j-log4j12-1.7.25.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\snappy-java-1.1.7.2.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\validation-api-1.1.0.Final.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zkclient-0.11.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zookeeper-3.4.13.jar;C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin\libs\zstd-jni-1.3.8-1.jar (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,459] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_111\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\Program Files\Common Files\Oracle\Java\javapath;C:\oracle\product\10.2.0\db_1\bin;C:\ProgramData\Oracle\Java\javapath;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\ATI Technologies\ATI.ACE\Core-Static;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Hewlett-Packard\SimplePass;C:\Program Files\Windows Live\Shared;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_111\bin;C:\Users\xqy\Downloads\apache-maven-3.5.0-bin\apache-maven-3.5.0\bin;C:\php\php\php-5.4.45;C:\php\php\php-5.4.45\ext;C:\Program Files\Google\Chrome\Application;C:\MinGW\bin;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Users\xqy\Desktop\Download\download\gradle-4.6\bin;C:\Program Files\Git\cmd;C:\Program Files\dotnet\;C:\Program Files\PxMEWNET;C:\Program Files\ojdkbuild\java-1.10.0-openjdk-1.10.0.1-1\bin;C:\Javalib\antlr-4.7.1-complete.jar;C:\Program Files\nodejs\;C:\Program Files (x86)\STMicroelectronics\STM32 ST-LINK Utility\ST-LINK Utility;C:\Users\xqy\Desktop\apache-tomcat-9.0.0.M22 - 副本\apache-ant-1.10.5\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\PuTTY\;C:\Program Files\sbt\bin;C:\Program Files\scala\bin;C:\Users\xqy\Desktop\projects\OpenCV\opencv\build\java\x86;C:\Program Files\OpenSSL-Win32\bin;C:\Program Files\GNU Tools ARM Embedded\8 2018-q4-major\bin;. (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,460] INFO Client environment:java.io.tmpdir=C:\Users\xqy\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:os.arch=x86 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:user.name=xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:user.home=C:\Users\xqy (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,464] INFO Client environment:user.dir=C:\Users\xqy\Desktop\github\dty717\kafka\kafka-bin (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,466] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@2e18f2 (org.apache.zookeeper.ZooKeeper)
[2019-06-06 11:15:05,494] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:15:05,513] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:15:05,515] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:15:05,515] INFO Accepted socket connection from /127.0.0.1:54581 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-06-06 11:15:05,541] INFO Client attempting to establish new session at /127.0.0.1:54581 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:15:05,542] INFO Creating new log file: log.90 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-06-06 11:15:05,615] INFO Established session 0x1000cae54780000 with negotiated timeout 6000 for client /127.0.0.1:54581 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-06-06 11:15:05,617] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000cae54780000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 11:15:05,620] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 11:15:05,842] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x1 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,028] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x2 zxid:0x92 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,068] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x3 zxid:0x93 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,140] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x4 zxid:0x94 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,197] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x5 zxid:0x95 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,251] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x6 zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,298] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x7 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,351] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x8 zxid:0x98 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,398] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0x9 zxid:0x99 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,452] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0xa zxid:0x9a txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,498] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0xb zxid:0x9b txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,552] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0xc zxid:0x9c txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,598] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:create cxid:0xd zxid:0x9d txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:06,905] INFO Cluster ID = LW9rTJf-QKWbIMgjpFjing (kafka.server.KafkaServer)
[2019-06-06 11:15:07,034] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 11:15:07,046] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.2-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.2-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = [DEFAULT]
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-06-06 11:15:07,146] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:15:07,147] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:15:07,147] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 11:15:07,269] INFO Loading logs. (kafka.log.LogManager)
[2019-06-06 11:15:07,519] INFO [Log partition=streams-wordcount-counts-store-changelog-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,533] INFO [Log partition=streams-wordcount-counts-store-changelog-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 211 ms (kafka.log.Log)
[2019-06-06 11:15:07,607] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,609] INFO [Log partition=__consumer_offsets-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:07,696] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,696] INFO [Log partition=__consumer_offsets-1, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 84 ms (kafka.log.Log)
[2019-06-06 11:15:07,785] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,786] INFO [Log partition=__consumer_offsets-10, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 88 ms (kafka.log.Log)
[2019-06-06 11:15:07,851] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,851] INFO [Log partition=__consumer_offsets-11, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 63 ms (kafka.log.Log)
[2019-06-06 11:15:07,918] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,919] INFO [Log partition=__consumer_offsets-12, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:07,984] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:07,985] INFO [Log partition=__consumer_offsets-13, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-06-06 11:15:08,062] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,063] INFO [Log partition=__consumer_offsets-14, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 76 ms (kafka.log.Log)
[2019-06-06 11:15:08,140] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,142] INFO [Log partition=__consumer_offsets-15, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-06-06 11:15:08,218] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,218] INFO [Log partition=__consumer_offsets-16, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 73 ms (kafka.log.Log)
[2019-06-06 11:15:08,296] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,296] INFO [Log partition=__consumer_offsets-17, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 76 ms (kafka.log.Log)
[2019-06-06 11:15:08,362] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,363] INFO [Log partition=__consumer_offsets-18, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:08,429] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,430] INFO [Log partition=__consumer_offsets-19, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:08,484] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,486] INFO [Log partition=__consumer_offsets-2, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-06-06 11:15:08,574] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,575] INFO [Log partition=__consumer_offsets-20, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 78 ms (kafka.log.Log)
[2019-06-06 11:15:08,674] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,675] INFO [Log partition=__consumer_offsets-21, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 93 ms (kafka.log.Log)
[2019-06-06 11:15:08,741] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,741] INFO [Log partition=__consumer_offsets-22, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:08,796] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,796] INFO [Log partition=__consumer_offsets-23, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-06-06 11:15:08,941] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Loading producer state till offset 5 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:08,988] INFO [ProducerStateManager partition=__consumer_offsets-24] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-24\00000000000000000005.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 11:15:09,027] INFO [Log partition=__consumer_offsets-24, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 5 in 229 ms (kafka.log.Log)
[2019-06-06 11:15:09,118] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,118] INFO [Log partition=__consumer_offsets-25, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 89 ms (kafka.log.Log)
[2019-06-06 11:15:09,173] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,174] INFO [Log partition=__consumer_offsets-26, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-06-06 11:15:09,240] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,241] INFO [Log partition=__consumer_offsets-27, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:09,307] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,308] INFO [Log partition=__consumer_offsets-28, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:09,396] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,397] INFO [Log partition=__consumer_offsets-29, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 88 ms (kafka.log.Log)
[2019-06-06 11:15:09,484] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,485] INFO [Log partition=__consumer_offsets-3, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 85 ms (kafka.log.Log)
[2019-06-06 11:15:09,540] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,541] INFO [Log partition=__consumer_offsets-30, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-06-06 11:15:09,607] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,608] INFO [Log partition=__consumer_offsets-31, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:09,696] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,696] INFO [Log partition=__consumer_offsets-32, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 87 ms (kafka.log.Log)
[2019-06-06 11:15:09,785] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,785] INFO [Log partition=__consumer_offsets-33, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 87 ms (kafka.log.Log)
[2019-06-06 11:15:09,840] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,841] INFO [Log partition=__consumer_offsets-34, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-06-06 11:15:09,909] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,912] INFO [ProducerStateManager partition=__consumer_offsets-35] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-35\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 11:15:09,912] INFO [Log partition=__consumer_offsets-35, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 70 ms (kafka.log.Log)
[2019-06-06 11:15:09,976] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:09,977] INFO [Log partition=__consumer_offsets-36, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-06-06 11:15:10,051] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,052] INFO [Log partition=__consumer_offsets-37, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 74 ms (kafka.log.Log)
[2019-06-06 11:15:10,118] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,119] INFO [Log partition=__consumer_offsets-38, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:10,174] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,175] INFO [Log partition=__consumer_offsets-39, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-06-06 11:15:10,242] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,243] INFO [Log partition=__consumer_offsets-4, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 67 ms (kafka.log.Log)
[2019-06-06 11:15:10,321] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,321] INFO [Log partition=__consumer_offsets-40, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 77 ms (kafka.log.Log)
[2019-06-06 11:15:10,396] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,396] INFO [Log partition=__consumer_offsets-41, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 74 ms (kafka.log.Log)
[2019-06-06 11:15:10,463] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,464] INFO [Log partition=__consumer_offsets-42, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:10,543] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,545] INFO [Log partition=__consumer_offsets-43, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 80 ms (kafka.log.Log)
[2019-06-06 11:15:10,609] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,609] INFO [Log partition=__consumer_offsets-44, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 63 ms (kafka.log.Log)
[2019-06-06 11:15:10,684] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,685] INFO [Log partition=__consumer_offsets-45, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 75 ms (kafka.log.Log)
[2019-06-06 11:15:10,762] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,763] INFO [Log partition=__consumer_offsets-46, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 76 ms (kafka.log.Log)
[2019-06-06 11:15:10,831] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,831] INFO [Log partition=__consumer_offsets-47, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-06-06 11:15:10,898] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,900] INFO [Log partition=__consumer_offsets-48, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 68 ms (kafka.log.Log)
[2019-06-06 11:15:10,985] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:10,986] INFO [Log partition=__consumer_offsets-49, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 84 ms (kafka.log.Log)
[2019-06-06 11:15:11,052] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:11,052] INFO [Log partition=__consumer_offsets-5, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-06-06 11:15:11,143] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:11,144] INFO [Log partition=__consumer_offsets-6, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 91 ms (kafka.log.Log)
[2019-06-06 11:15:11,209] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:11,209] INFO [Log partition=__consumer_offsets-7, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-06-06 11:15:11,329] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Loading producer state till offset 5 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:11,332] INFO [ProducerStateManager partition=__consumer_offsets-8] Loading producer state from snapshot file 'C:\tmp\kafka-logs\__consumer_offsets-8\00000000000000000005.snapshot' (kafka.log.ProducerStateManager)
[2019-06-06 11:15:11,333] INFO [Log partition=__consumer_offsets-8, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 5 in 123 ms (kafka.log.Log)
[2019-06-06 11:15:11,407] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:15:11,407] INFO [Log partition=__consumer_offsets-9, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 73 ms (kafka.log.Log)
[2019-06-06 11:15:11,420] INFO Logs loading complete in 4151 ms. (kafka.log.LogManager)
[2019-06-06 11:15:11,456] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-06-06 11:15:11,458] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-06-06 11:15:11,983] INFO Awaiting socket connections on s0.0.0.0:9092. (kafka.network.Acceptor)
[2019-06-06 11:15:12,084] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2019-06-06 11:15:12,086] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2019-06-06 11:15:12,155] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,155] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,155] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,157] INFO [ExpirationReaper-0-ElectPreferredLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,171] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 11:15:12,251] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-06-06 11:15:12,316] INFO Stat of the created znode at /brokers/ids/0 is: 158,158,1559790912263,1559790912263,1,0,0,72071536918921216,182,0,158
 (kafka.zk.KafkaZkClient)
[2019-06-06 11:15:12,319] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(xqy-HP,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 158 (kafka.zk.KafkaZkClient)
[2019-06-06 11:15:12,432] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,439] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,439] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 11:15:12,545] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:15:12,547] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:15:12,572] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 22 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:12,654] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2 (kafka.coordinator.transaction.ProducerIdManager)
[2019-06-06 11:15:12,713] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:15:12,715] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 11:15:12,715] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 11:15:12,867] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 11:15:12,904] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:multi cxid:0x64 zxid:0xa1 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:15:12,988] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2019-06-06 11:15:13,043] INFO Kafka version: 2.2.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 11:15:13,044] INFO Kafka commitId: 05fcfde8f69b0349 (org.apache.kafka.common.utils.AppInfoParser)
[2019-06-06 11:15:13,048] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-06-06 11:15:13,130] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, streams-wordcount-counts-store-changelog-0, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:15:13,146] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,153] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,258] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,259] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,334] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,335] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,390] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,390] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,464] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,464] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,532] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,532] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,587] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,587] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,645] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,645] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,701] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,701] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,756] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,757] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,832] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,832] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,887] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,888] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:13,956] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:13,956] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,011] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,012] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,068] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,069] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,122] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,123] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,187] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,187] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,243] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,244] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,299] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,300] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,355] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,356] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,422] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,423] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,479] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,479] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,553] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 5 (kafka.cluster.Replica)
[2019-06-06 11:15:14,553] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 5. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,556] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 5 (kafka.cluster.Replica)
[2019-06-06 11:15:14,556] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 5. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,558] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,558] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,631] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,632] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,699] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,699] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,755] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,755] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:14,944] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:14,945] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,052] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,052] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,240] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,240] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,345] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,346] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,432] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,432] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,508] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,510] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,566] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,567] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,620] INFO Replica loaded for partition streams-wordcount-counts-store-changelog-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,621] INFO [Partition streams-wordcount-counts-store-changelog-0 broker=0] streams-wordcount-counts-store-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,681] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,682] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,750] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,751] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,823] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,823] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,898] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,898] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:15,953] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:15,954] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,020] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 3 (kafka.cluster.Replica)
[2019-06-06 11:15:16,021] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,023] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,023] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,080] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,081] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,158] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,196] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,374] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,374] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,564] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,564] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,620] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,621] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,676] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,676] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,733] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,733] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,788] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:15:16,789] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:15:16,850] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,852] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,854] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,855] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,856] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,857] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,858] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,978] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 126 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,979] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,979] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,979] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,980] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,981] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,982] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:16,983] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,009] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 26 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,010] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,010] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,011] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,016] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,016] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,016] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,016] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,017] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,023] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,025] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,025] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,025] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,025] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,026] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,026] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,026] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:15:17,026] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:16:52,898] INFO Creating topic streams-plaintext-input with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:16:52,899] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:setData cxid:0xa0 zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/config/topics/streams-plaintext-input Error:KeeperErrorCode = NoNode for /config/topics/streams-plaintext-input (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:16:53,258] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-plaintext-input-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:16:53,316] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:16:53,320] INFO [Log partition=streams-plaintext-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 60 ms (kafka.log.Log)
[2019-06-06 11:16:53,321] INFO Created log for partition streams-plaintext-input-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:16:53,322] INFO [Partition streams-plaintext-input-0 broker=0] No checkpointed highwatermark is found for partition streams-plaintext-input-0 (kafka.cluster.Partition)
[2019-06-06 11:16:53,322] INFO Replica loaded for partition streams-plaintext-input-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:16:53,322] INFO [Partition streams-plaintext-input-0 broker=0] streams-plaintext-input-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:17:52,875] INFO Creating topic streams-wordcount-output with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:17:52,876] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:setData cxid:0xaa zxid:0xa8 txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-output Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-output (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:17:53,187] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-output-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:17:53,242] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:17:53,244] INFO [Log partition=streams-wordcount-output-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-06-06 11:17:53,245] INFO Created log for partition streams-wordcount-output-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:17:53,246] INFO [Partition streams-wordcount-output-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-output-0 (kafka.cluster.Partition)
[2019-06-06 11:17:53,246] INFO Replica loaded for partition streams-wordcount-output-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:17:53,246] INFO [Partition streams-wordcount-output-0 broker=0] streams-wordcount-output-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:19:11,473] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member streams-wordcount-2f7fe014-03a2-4d4d-86f0-cf50096d4e2f-StreamThread-1-consumer-5faef532-5217-411e-9ffd-828423df02d8) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:19:11,479] INFO [GroupCoordinator 0]: Stabilized group streams-wordcount generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:19:11,525] INFO Creating topic streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition with configuration {segment.bytes=52428800, retention.ms=9223372036854775807, segment.index.bytes=52428800, segment.ms=600000, cleanup.policy=delete} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:19:11,525] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:setData cxid:0xb8 zxid:0xae txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:19:11,864] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:19:11,952] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:19:11,954] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[2019-06-06 11:19:11,955] INFO Created log for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> delete, flush.ms -> 9223372036854775807, segment.ms -> 600000, segment.bytes -> 52428800, retention.ms -> 9223372036854775807, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 52428800, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:19:11,956] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 (kafka.cluster.Partition)
[2019-06-06 11:19:11,956] INFO Replica loaded for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:19:11,956] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 broker=0] streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:19:12,103] INFO Creating topic streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 11:19:12,104] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:setData cxid:0xc2 zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 11:19:12,441] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 11:19:12,448] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 11:19:12,450] INFO [Log partition=streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-06-06 11:19:12,451] INFO Created log for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 11:19:12,452] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 (kafka.cluster.Partition)
[2019-06-06 11:19:12,452] INFO Replica loaded for partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 11:19:12,453] INFO [Partition streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 broker=0] streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 11:19:12,602] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-wordcount for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:19:22,666] INFO [GroupCoordinator 0]: Member streams-wordcount-2f7fe014-03a2-4d4d-86f0-cf50096d4e2f-StreamThread-1-consumer-5faef532-5217-411e-9ffd-828423df02d8 in group streams-wordcount has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:19:22,667] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member streams-wordcount-2f7fe014-03a2-4d4d-86f0-cf50096d4e2f-StreamThread-1-consumer-5faef532-5217-411e-9ffd-828423df02d8 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:19:22,668] INFO [GroupCoordinator 0]: Group streams-wordcount with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 11:25:12,550] INFO [GroupMetadataManager brokerId=0] Group streams-wordcount transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:25:12,565] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 18 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:35:12,571] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:45:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 11:55:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:05:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:15:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:25:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:35:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:45:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 12:55:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:05:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:15:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:25:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:35:12,572] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:44:03,895] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 0 (__consumer_offsets-35) (reason: Adding new member streams-wordcount-c53c8588-6c96-42c9-9e58-8c3f7c2a2f11-StreamThread-1-consumer-b2231ec3-7bff-4e88-b3a0-5da4ca404e87) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:44:03,926] INFO [GroupCoordinator 0]: Stabilized group streams-wordcount generation 1 (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:44:04,005] INFO [GroupCoordinator 0]: Assignment received from leader for group streams-wordcount for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:44:14,088] INFO [GroupCoordinator 0]: Member streams-wordcount-c53c8588-6c96-42c9-9e58-8c3f7c2a2f11-StreamThread-1-consumer-b2231ec3-7bff-4e88-b3a0-5da4ca404e87 in group streams-wordcount has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:44:14,089] INFO [GroupCoordinator 0]: Preparing to rebalance group streams-wordcount in state PreparingRebalance with old generation 1 (__consumer_offsets-35) (reason: removing member streams-wordcount-c53c8588-6c96-42c9-9e58-8c3f7c2a2f11-StreamThread-1-consumer-b2231ec3-7bff-4e88-b3a0-5da4ca404e87 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:44:14,089] INFO [GroupCoordinator 0]: Group streams-wordcount with generation 2 is now empty (__consumer_offsets-35) (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 13:45:12,546] INFO [GroupMetadataManager brokerId=0] Group streams-wordcount transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:45:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 13:55:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:05:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:15:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:25:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:35:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:45:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 14:55:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:03:45,592] INFO Creating topic streams-wordcount-input with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-06-06 15:03:45,752] INFO Got user-level KeeperException when processing sessionid:0x1000cae54780000 type:setData cxid:0xce zxid:0xba txntype:-1 reqpath:n/a Error Path:/config/topics/streams-wordcount-input Error:KeeperErrorCode = NoNode for /config/topics/streams-wordcount-input (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-06-06 15:03:46,751] INFO [KafkaApi-0] Auto creation of topic streams-wordcount-input with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-06-06 15:03:47,359] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(streams-wordcount-input-0) (kafka.server.ReplicaFetcherManager)
[2019-06-06 15:03:47,600] INFO [Log partition=streams-wordcount-input-0, dir=C:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-06-06 15:03:47,602] INFO [Log partition=streams-wordcount-input-0, dir=C:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 110 ms (kafka.log.Log)
[2019-06-06 15:03:47,603] INFO Created log for partition streams-wordcount-input-0 in C:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.2-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-06-06 15:03:47,628] INFO [Partition streams-wordcount-input-0 broker=0] No checkpointed highwatermark is found for partition streams-wordcount-input-0 (kafka.cluster.Partition)
[2019-06-06 15:03:47,628] INFO Replica loaded for partition streams-wordcount-input-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-06-06 15:03:47,629] INFO [Partition streams-wordcount-input-0 broker=0] streams-wordcount-input-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-06-06 15:05:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:15:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:25:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:35:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:45:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 15:55:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:05:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:15:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:25:12,547] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:35:12,548] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:45:12,575] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 29 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:55:12,546] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-06-06 16:58:24,490] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-06-06 16:58:25,477] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-06-06 16:58:29,729] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2019-06-06 16:58:30,319] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 16:58:30,523] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 16:58:30,523] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-06-06 16:58:30,570] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-06-06 16:58:30,958] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-06-06 16:58:31,032] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-06-06 16:58:31,324] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-06-06 16:58:31,909] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-06-06 16:58:32,325] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,503] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,503] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,614] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 16:58:32,713] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 1000 (kafka.coordinator.transaction.ProducerIdManager)
[2019-06-06 16:58:32,761] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-06-06 16:58:32,761] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 16:58:32,835] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 16:58:32,835] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-06-06 16:58:32,836] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-06-06 16:58:32,837] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 16:58:32,838] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,888] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,888] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:32,888] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:33,088] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:33,088] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:33,089] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-06-06 16:58:33,173] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-06-06 16:58:33,200] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 16:58:33,201] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 16:58:33,201] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-06-06 16:58:33,217] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-06-06 16:58:34,568] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-06-06 16:58:34,570] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 16:58:34,572] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-06-06 16:58:34,572] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,766] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,766] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,767] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,966] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,966] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:34,966] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,167] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,167] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,168] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,369] INFO [ExpirationReaper-0-ElectPreferredLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,369] INFO [ExpirationReaper-0-ElectPreferredLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-06-06 16:58:35,477] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-06-06 16:58:35,478] INFO Shutting down. (kafka.log.LogManager)
[2019-06-06 16:58:36,798] INFO [ProducerStateManager partition=streams-wordcount-input-0] Writing producer snapshot at offset 10 (kafka.log.ProducerStateManager)
[2019-06-06 16:58:37,265] INFO [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2019-06-06 16:58:41,616] INFO Shutdown complete. (kafka.log.LogManager)
[2019-06-06 16:58:42,571] WARN Client session timed out, have not heard from server in 4000ms for sessionid 0x1000cae54780000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 16:58:42,571] INFO Client session timed out, have not heard from server in 4000ms for sessionid 0x1000cae54780000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-06-06 16:58:42,861] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 16:58:44,095] INFO Session: 0x1000cae54780000 closed (org.apache.zookeeper.ZooKeeper)
[2019-06-06 16:58:44,095] INFO EventThread shut down for session: 0x1000cae54780000 (org.apache.zookeeper.ClientCnxn)
[2019-06-06 16:58:44,096] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-06-06 16:58:44,235] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:44,606] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:44,606] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:44,606] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:45,607] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:45,607] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:45,608] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:46,607] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:46,607] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-06-06 16:58:46,608] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-06-06 16:58:46,680] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-06-06 16:58:46,683] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
